"""
í•œêµ­ì–´ ì—ì„¸ì´ ë°ì´í„°ì…‹ì„ ìœ„í•œ 11ê°œ í‰ê°€ ê¸°ì¤€ ë©€í‹°-íƒœìŠ¤í¬ íšŒê·€ ëª¨ë¸ ì‚¬ìš© ì˜ˆì œ
"""

import pandas as pd
import numpy as np
from document_bert_architectures import DocumentBertSentenceChunkAttentionLSTM, DocumentBertCombineWordDocumentLinear
from encoder import encode_documents_by_sentence, encode_documents_full_text
from evaluate import evaluation_multi_regression, print_evaluation_results, plot_evaluation_results, analyze_prediction_errors
import torch

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

# 11ê°œ í‰ê°€ ê¸°ì¤€ ì •ì˜
CRITERION_NAMES = [
            'ë¬¸ë²• ì •í™•ë„', 'ë‹¨ì–´ ì„ íƒì˜ ì ì ˆì„±', 'ë¬¸ì¥ í‘œí˜„', 'ë¬¸ë‹¨ ë‚´ êµ¬ì¡°', 'ë¬¸ë‹¨ ê°„ êµ¬ì¡°',
            'êµ¬ì¡°ì˜ ì¼ê´€ì„±', 'ë¶„ëŸ‰ì˜ ì ì ˆì„±', 'ì£¼ì œ ëª…ë£Œì„±', 'ì°½ì˜ì„±', 
            'í”„ë¡¬í”„íŠ¸ ë…í•´ë ¥', 'ì„¤ëª…ì˜ êµ¬ì²´ì„±'
]

def create_sample_multi_data():
    """ë©€í‹°-íƒœìŠ¤í¬ íšŒê·€ë¥¼ ìœ„í•œ ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
    essays = [
        "í™˜ê²½ ë³´í˜¸ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ë…¼ì˜í•´ë³´ê² ìŠµë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ì§€êµ¬ ì˜¨ë‚œí™”ëŠ” í˜„ì¬ ì¸ë¥˜ê°€ ì§ë©´í•œ ê°€ì¥ ì‹¬ê°í•œ ë¬¸ì œ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ì´ì‚°í™”íƒ„ì†Œ ë°°ì¶œëŸ‰ì„ ì¤„ì´ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ë°©ì•ˆì´ í•„ìš”í•©ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ê°œì¸ê³¼ ê¸°ì—…, ì •ë¶€ê°€ ëª¨ë‘ í˜‘ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.",
        
        "êµìœ¡ ì‹œìŠ¤í…œì˜ í˜ì‹ ì´ í•„ìš”í•©ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#21ì„¸ê¸° êµìœ¡ì€ ë‹¨ìˆœí•œ ì§€ì‹ ì „ë‹¬ì„ ë„˜ì–´ì„œì•¼ í•©ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ì°½ì˜ì  ì‚¬ê³ ì™€ ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì„ ê¸°ë¥´ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ê¸°ìˆ ì„ í™œìš©í•œ ë§ì¶¤í˜• êµìœ¡ì´ í•´ë‹µì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        
        "ì¸ê³µì§€ëŠ¥ì˜ ë°œë‹¬ì´ ì‚¬íšŒì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#AI ê¸°ìˆ ì€ ìš°ë¦¬ ìƒí™œì˜ ë§ì€ ë¶€ë¶„ì„ ë³€í™”ì‹œí‚¤ê³  ìˆìŠµë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ì¼ìë¦¬ ë³€í™”ì™€ ìƒˆë¡œìš´ ê¸°íšŒ ì°½ì¶œì´ ë™ì‹œì— ì¼ì–´ë‚˜ê³  ìˆìŠµë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ìœ¤ë¦¬ì  ê³ ë ¤ì‚¬í•­ë„ í•¨ê»˜ ë…¼ì˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.",
        
        "ê±´ê°•í•œ ë¼ì´í”„ìŠ¤íƒ€ì¼ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ê·œì¹™ì ì¸ ìš´ë™ê³¼ ê· í˜• ì¡íŒ ì‹ë‹¨ì´ ê¸°ë³¸ì…ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ì •ì‹  ê±´ê°•ë„ ì‹ ì²´ ê±´ê°•ë§Œí¼ ì¤‘ìš”í•©ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬ì™€ ì¶©ë¶„í•œ íœ´ì‹ì´ í•„ìš”í•©ë‹ˆë‹¤.",
        
        "ë¬¸í™” ë‹¤ì–‘ì„±ê³¼ ìƒí˜¸ ì´í•´ì— ëŒ€í•´ ìƒê°í•´ë´…ì‹œë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ì„œë¡œ ë‹¤ë¥¸ ë¬¸í™”ì  ë°°ê²½ì„ ê°€ì§„ ì‚¬ëŒë“¤ê³¼ì˜ ì†Œí†µì´ ì¤‘ìš”í•©ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#í¸ê²¬ì„ ë²„ë¦¬ê³  ì—´ë¦° ë§ˆìŒìœ¼ë¡œ ì ‘ê·¼í•´ì•¼ í•©ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ë‹¤ì–‘ì„±ì´ ì‚¬íšŒë¥¼ ë”ìš± í’ìš”ë¡­ê²Œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.",
        
        "ê²½ì œì  ë¶ˆí‰ë“± í•´ì†Œ ë°©ì•ˆì„ ëª¨ìƒ‰í•´ë´…ì‹œë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ì†Œë“ ê²©ì°¨ê°€ ì ì  ë²Œì–´ì§€ê³  ìˆëŠ” í˜„ì‹¤ì…ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#êµìœ¡ ê¸°íšŒì˜ í‰ë“±ê³¼ ê³µì •í•œ ê²½ìŸì´ í•„ìš”í•©ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ì‚¬íšŒ ì•ˆì „ë§ êµ¬ì¶•ì´ ì‹œê¸‰í•œ ê³¼ì œì…ë‹ˆë‹¤."
    ]
    
    # ê° ì—ì„¸ì´ì— ëŒ€í•´ 11ê°œ í‰ê°€ ê¸°ì¤€ì˜ ì ìˆ˜ (0~3 ë²”ìœ„)
    # ë…¼ë¦¬ì„±, ì°½ì˜ì„±, ì„¤ë“ë ¥, ê·¼ê±°ì˜í’ë¶€í•¨, êµ¬ì„±ì˜ì™„ì„±ë„, í‘œí˜„ì˜ì •í™•ì„±, ì–´íœ˜ì˜ë‹¤ì–‘ì„±, ë¬¸ë²•ì˜ì •í™•ì„±, ë‚´ìš©ì˜ê¹Šì´, ë…ì°½ì„±, ì „ì²´ì ì¼ê´€ì„±
    labels = [
        [2.5, 2.0, 2.3, 2.1, 2.4, 2.2, 2.0, 2.3, 2.2, 1.8, 2.3],  # í™˜ê²½ ë³´í˜¸ ì—ì„¸ì´
        [2.8, 2.5, 2.6, 2.4, 2.7, 2.5, 2.3, 2.4, 2.6, 2.2, 2.5],  # êµìœ¡ í˜ì‹  ì—ì„¸ì´
        [2.3, 2.8, 2.4, 2.2, 2.1, 2.0, 2.4, 2.1, 2.5, 2.6, 2.2],  # AI ì˜í–¥ ì—ì„¸ì´
        [2.0, 1.8, 2.1, 2.0, 2.2, 2.3, 1.9, 2.2, 2.0, 1.7, 2.1],  # ê±´ê°• ë¼ì´í”„ìŠ¤íƒ€ì¼ ì—ì„¸ì´
        [2.4, 2.3, 2.5, 2.2, 2.3, 2.4, 2.1, 2.3, 2.3, 2.0, 2.4],  # ë¬¸í™” ë‹¤ì–‘ì„± ì—ì„¸ì´
        [2.6, 2.1, 2.7, 2.5, 2.4, 2.2, 2.2, 2.1, 2.4, 2.0, 2.3]   # ê²½ì œ ë¶ˆí‰ë“± ì—ì„¸ì´
    ]
    
    return essays, labels

# 1. ë°ì´í„° ì¤€ë¹„
print("=== ë©€í‹°-íƒœìŠ¤í¬ íšŒê·€ ë°ì´í„° ì¤€ë¹„ ===")
essays, labels = create_sample_multi_data()
print(f"ì—ì„¸ì´ ìˆ˜: {len(essays)}")
print(f"ë¼ë²¨ í˜•íƒœ: {np.array(labels).shape}")  # (N, 11)
print(f"í‰ê°€ ê¸°ì¤€: {CRITERION_NAMES}")
print(f"\nì²« ë²ˆì§¸ ì—ì„¸ì´ ì ìˆ˜:")
for i, criterion in enumerate(CRITERION_NAMES):
    print(f"  {criterion}: {labels[0][i]:.1f}ì ")

# DataFrameìœ¼ë¡œ ë³€í™˜
df_essays = pd.DataFrame({'essay': essays})
df_labels = pd.DataFrame(labels, columns=CRITERION_NAMES)

print(f"\në¼ë²¨ í†µê³„:")
print(df_labels.describe())

# 2. ëª¨ë¸ ì´ˆê¸°í™”
print("\n=== ëª¨ë¸ ì´ˆê¸°í™” ===")
from model_architecture_bert_multi_scale_multi_loss import DocumentBertScoringModel

class Args:
    def __init__(self):
        self.device = device
        self.batch_size = 2
        self.model_directory = './models'
        self.result_file = 'multi_regression_result.txt'

args = Args()
model = DocumentBertScoringModel(load_model=False, args=args)
print("ë©€í‹°-íƒœìŠ¤í¬ íšŒê·€ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")

# 3. ë°ì´í„° ì¸ì½”ë”© í…ŒìŠ¤íŠ¸  
print("\n=== ë°ì´í„° ì¸ì½”ë”© í…ŒìŠ¤íŠ¸ ===")
sentence_encoded, sentence_lengths = encode_documents_by_sentence(
    essays, model.bert_tokenizer, max_input_length=128)
print(f"ë¬¸ì¥ë³„ ì¸ì½”ë”© ê²°ê³¼: {sentence_encoded.shape}")
print(f"ê° ë¬¸ì„œì˜ ë¬¸ì¥ ìˆ˜: {sentence_lengths.tolist()}")

full_encoded, full_lengths = encode_documents_full_text(
    essays, model.bert_tokenizer, max_input_length=512)
print(f"ì „ì²´ ë¬¸ì„œ ì¸ì½”ë”© ê²°ê³¼: {full_encoded.shape}")

# 4. ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
print("\n=== ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ===")
test_essay = "ë””ì§€í„¸ ì‹œëŒ€ì˜ êµìœ¡ í˜ì‹ ì´ í•„ìš”í•©ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ì˜¨ë¼ì¸ í•™ìŠµ í”Œë«í¼ì´ ë°œë‹¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ê°œì¸ ë§ì¶¤í˜• êµìœ¡ì´ ê°€ëŠ¥í•´ì¡ŒìŠµë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#í•˜ì§€ë§Œ ë””ì§€í„¸ ê²©ì°¨ ë¬¸ì œë„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤."

result_dict, predictions = model.predict_single(test_essay)
print("ì˜ˆì¸¡ ê²°ê³¼:")
for criterion, score in result_dict.items():
    print(f"  {criterion}: {score:.3f}ì ")

# 5. í‰ê°€ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸
print("\n=== í‰ê°€ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ===")
# ê°€ìƒì˜ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„± (ì‹¤ì œë¡œëŠ” ëª¨ë¸ì—ì„œ ë‚˜ì˜´)
true_labels_array = np.array(labels)
# ì‹¤ì œ ê°’ì— ì•½ê°„ì˜ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•´ì„œ ê°€ìƒ ì˜ˆì¸¡ê°’ ìƒì„±
pred_labels_array = true_labels_array + np.random.normal(0, 0.2, true_labels_array.shape)
pred_labels_array = np.clip(pred_labels_array, 0, 3)  # 0-3 ë²”ìœ„ë¡œ í´ë¦¬í•‘

# í‰ê°€ ìˆ˜í–‰
results = evaluation_multi_regression(true_labels_array, pred_labels_array, CRITERION_NAMES)
print_evaluation_results(results, detailed=True)

# 6. ì‹œê°í™” (matplotlib í•„ìš”)
print("\n=== ê²°ê³¼ ì‹œê°í™” ===")
try:
    plot_evaluation_results(results, save_path='evaluation_results.png')
    print("ì‹œê°í™” ì™„ë£Œ: evaluation_results.png ì €ì¥ë¨")
except ImportError:
    print("matplotlibê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

# 7. ì˜¤ì°¨ ë¶„ì„
print("\n=== ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„ì„ ===")
analyze_prediction_errors(true_labels_array, pred_labels_array, CRITERION_NAMES, top_k=3)

# 8. ì‹¤ì œ í•™ìŠµ ì˜ˆì œ
def train_model_example():
    """ì‹¤ì œ ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµí•˜ëŠ” ì˜ˆì œ"""
    print("\n=== ëª¨ë¸ í•™ìŠµ ì˜ˆì œ ===")
    
    # ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤
    # í•™ìŠµ ë°ì´í„° ì¤€ë¹„
    train_data = (df_essays['essay'], df_labels)
    
    print("í•™ìŠµ ë°ì´í„° í˜•íƒœ:")
    print(f"  ì—ì„¸ì´: {len(train_data[0])}")
    print(f"  ë¼ë²¨: {train_data[1].shape}")
    
    # í•™ìŠµ ì‹¤í–‰ (ì‹¤ì œë¡œëŠ” ë” ë§ì€ ë°ì´í„°ì™€ ì—í¬í¬ í•„ìš”)
    print("\nì‹¤ì œ í•™ìŠµì„ ìœ„í•´ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ í˜¸ì¶œ:")
    print("model.fit(train_data, test=test_data, mode='korean_multi_regression')")
    
    # ì£¼ì„ í•´ì œí•˜ì—¬ ì‹¤ì œ í•™ìŠµ ì‹¤í–‰ ê°€ëŠ¥
    model.fit(train_data, test=train_data, mode='korean_multi_regression')

train_model_example()


# 9. ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ ì˜ˆì œ
def save_load_model_example():
    """ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ ì˜ˆì œ"""
    print("\n=== ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ ì˜ˆì œ ===")
    
    # ëª¨ë¸ ì €ì¥
    save_dir_word = './saved_models/multi_regression_word_doc_model'
    save_dir_chunk = './saved_models/multi_regression_chunk_model'
    
    print("ëª¨ë¸ ì €ì¥:")
    print(f"  Word Document Model: {save_dir_word}")
    print(f"  Chunk Model: {save_dir_chunk}")
    
    # ì‹¤ì œ ì €ì¥ (ì£¼ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©)
    # model.bert_regression_by_word_document.save_pretrained(save_dir_word)
    # model.bert_regression_by_chunk.save_pretrained(save_dir_chunk)
    
    # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ
    print("\nì €ì¥ëœ ëª¨ë¸ ë¡œë“œ:")
    print("""
    loaded_model = DocumentBertScoringModel(
        load_model=True,
        word_doc_model_path='./saved_models/multi_regression_word_doc_model',
        chunk_model_path='./saved_models/multi_regression_chunk_model',
        args=args
    )
    """)

# 10. ë°°ì¹˜ ì˜ˆì¸¡ í•¨ìˆ˜
def batch_predict_multi(model, essays, batch_size=4):
    """ì—¬ëŸ¬ ì—ì„¸ì´ì— ëŒ€í•œ ë°°ì¹˜ ì˜ˆì¸¡ (11ê°œ í‰ê°€ ê¸°ì¤€)"""
    all_predictions = []
    
    for i in range(0, len(essays), batch_size):
        batch_essays = essays[i:i+batch_size]
        batch_predictions = []
        
        for essay in batch_essays:
            _, predictions = model.predict_single(essay)
            batch_predictions.append(predictions)
        
        all_predictions.extend(batch_predictions)
    
    return np.array(all_predictions)

# 11. ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_multi_regression_data(essays, labels_matrix):
    """ë©€í‹°-íƒœìŠ¤í¬ íšŒê·€ë¥¼ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬"""
    processed_essays = []
    processed_labels = []
    
    for essay, label_row in zip(essays, labels_matrix):
        # ì—ì„¸ì´ ì „ì²˜ë¦¬
        essay = essay.strip()
        if "#@ë¬¸ì¥êµ¬ë¶„#" not in essay:
            # ê°„ë‹¨í•œ ë¬¸ì¥ ë¶„í•  (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ ë¶„í•  í•„ìš”)
            sentences = essay.split('. ')
            essay = "#@ë¬¸ì¥êµ¬ë¶„#".join(sentences)
        
        # ë¼ë²¨ ê²€ì¦ (0~3 ë²”ìœ„, 11ê°œ)
        label_row = np.array(label_row)
        if len(label_row) != 11:
            print(f"ê²½ê³ : ë¼ë²¨ ê°œìˆ˜ê°€ 11ê°œê°€ ì•„ë‹™ë‹ˆë‹¤. ({len(label_row)}ê°œ)")
            continue
        
        # 0~3 ë²”ìœ„ë¡œ í´ë¦¬í•‘
        label_row = np.clip(label_row, 0.0, 3.0)
        
        processed_essays.append(essay)
        processed_labels.append(label_row.tolist())
    
    return processed_essays, processed_labels

# 12. CSV íŒŒì¼ ì²˜ë¦¬ ì˜ˆì œ
def load_csv_multi_regression_example():
    """CSV íŒŒì¼ì—ì„œ ë©€í‹°-íƒœìŠ¤í¬ íšŒê·€ ë°ì´í„° ë¡œë“œ ì˜ˆì œ"""
    print("\n=== CSV íŒŒì¼ ë¡œë“œ ì˜ˆì œ ===")
    
    # ì˜ˆìƒë˜ëŠ” CSV íŒŒì¼ í˜•íƒœ
    sample_csv_structure = """
    ì˜ˆìƒ CSV êµ¬ì¡°:
    essay_text,ë…¼ë¦¬ì„±,ì°½ì˜ì„±,ì„¤ë“ë ¥,ê·¼ê±°ì˜í’ë¶€í•¨,êµ¬ì„±ì˜ì™„ì„±ë„,í‘œí˜„ì˜ì •í™•ì„±,ì–´íœ˜ì˜ë‹¤ì–‘ì„±,ë¬¸ë²•ì˜ì •í™•ì„±,ë‚´ìš©ì˜ê¹Šì´,ë…ì°½ì„±,ì „ì²´ì ì¼ê´€ì„±
    "ì•ˆë…•í•˜ì„¸ìš”.#@ë¬¸ì¥êµ¬ë¶„#ì˜¤ëŠ˜ì€...",2.5,2.0,2.3,2.1,2.4,2.2,2.0,2.3,2.2,1.8,2.3
    "êµìœ¡ì˜ ì¤‘ìš”ì„±ì—...",2.8,2.5,2.6,2.4,2.7,2.5,2.3,2.4,2.6,2.2,2.5
    ...
    """
    print(sample_csv_structure)
    
    # ì‹¤ì œ CSV ë¡œë“œ ì½”ë“œ ì˜ˆì œ
    load_code_example = """
    # ì‹¤ì œ CSV íŒŒì¼ ë¡œë“œ ë°©ë²•:
    df = pd.read_csv('korean_essays_multi_regression.csv')
    
    essays = df['essay_text'].tolist()
    label_columns = ['ë…¼ë¦¬ì„±', 'ì°½ì˜ì„±', 'ì„¤ë“ë ¥', 'ê·¼ê±°ì˜í’ë¶€í•¨', 'êµ¬ì„±ì˜ì™„ì„±ë„',
                    'í‘œí˜„ì˜ì •í™•ì„±', 'ì–´íœ˜ì˜ë‹¤ì–‘ì„±', 'ë¬¸ë²•ì˜ì •í™•ì„±', 'ë‚´ìš©ì˜ê¹Šì´', 
                    'ë…ì°½ì„±', 'ì „ì²´ì ì¼ê´€ì„±']
    labels = df[label_columns].values  # (N, 11) numpy array
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    processed_essays, processed_labels = preprocess_multi_regression_data(essays, labels)
    
    # DataFrameìœ¼ë¡œ ë³€í™˜
    df_essays = pd.Series(processed_essays)
    df_labels = pd.DataFrame(processed_labels, columns=label_columns)
    
    # ëª¨ë¸ í•™ìŠµ
    model = DocumentBertScoringModel(load_model=False, args=args)
    model.fit((df_essays, df_labels), mode='korean_multi_regression')
    """
    print(load_code_example)

# 13. ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
def monitor_training_progress():
    """í•™ìŠµ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì˜ˆì œ"""
    print("\n=== í•™ìŠµ ì§„í–‰ ëª¨ë‹ˆí„°ë§ ===")
    
    monitoring_code = """
    # í•™ìŠµ ì¤‘ ì†ì‹¤ ë° ë©”íŠ¸ë¦­ ëª¨ë‹ˆí„°ë§
    import matplotlib.pyplot as plt
    
    # ì €ì¥ëœ í•™ìŠµ ê²°ê³¼ ë¡œë“œ
    train_losses = np.load('./train_valid_loss/korean_multi_regression_loss.npy')
    train_mse = np.load('./train_valid_loss/korean_multi_regression_mse.npy')
    train_mae = np.load('./train_valid_loss/korean_multi_regression_mae.npy')
    
    # í•™ìŠµ ê³¡ì„  ê·¸ë¦¬ê¸°
    plt.figure(figsize=(15, 5))
    
    plt.subplot(1, 3, 1)
    plt.plot(train_losses)
    plt.title('Training Loss')
    plt.xlabel('Iteration')
    plt.ylabel('Loss')
    
    plt.subplot(1, 3, 2)
    plt.plot(train_mse)
    plt.title('Validation MSE')
    plt.xlabel('Epoch')
    plt.ylabel('MSE')
    
    plt.subplot(1, 3, 3)
    plt.plot(train_mae)
    plt.title('Validation MAE')
    plt.xlabel('Epoch')
    plt.ylabel('MAE')
    
    plt.tight_layout()
    plt.savefig('training_progress.png')
    plt.show()
    """
    print(monitoring_code)

# ì‹¤í–‰ ì˜ˆì œë“¤
if __name__ == "__main__":
    # ì „ì²˜ë¦¬ ì˜ˆì œ
    processed_essays, processed_labels = preprocess_multi_regression_data(essays, labels)
    print(f"\nì „ì²˜ë¦¬ëœ ë°ì´í„°: ì—ì„¸ì´ {len(processed_essays)}ê°œ, ë¼ë²¨ í˜•íƒœ {np.array(processed_labels).shape}")
    
    # ë°°ì¹˜ ì˜ˆì¸¡ ì˜ˆì œ
    print("\n=== ë°°ì¹˜ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸ ===")
    batch_predictions = batch_predict_multi(model, processed_essays[:3])
    print(f"ë°°ì¹˜ ì˜ˆì¸¡ ê²°ê³¼ í˜•íƒœ: {batch_predictions.shape}")
    print("ì²« ë²ˆì§¸ ì—ì„¸ì´ ì˜ˆì¸¡ ì ìˆ˜:")
    for i, criterion in enumerate(CRITERION_NAMES):
        print(f"  {criterion}: {batch_predictions[0][i]:.3f}ì ")
    
    # ë‹¤ë¥¸ ì˜ˆì œë“¤ ì‹¤í–‰
    train_model_example()
    save_load_model_example()
    load_csv_multi_regression_example()
    monitor_training_progress()
    
    print("\n" + "="*60)
    print("ë©€í‹°-íƒœìŠ¤í¬ íšŒê·€ ëª¨ë¸ ì‚¬ìš© ê°€ì´ë“œ")
    print("="*60)
    
    print("""
ğŸ“‹ ì£¼ìš” íŠ¹ì§•:
  â€¢ í•˜ë‚˜ì˜ ì—ì„¸ì´ â†’ 11ê°œ í‰ê°€ ê¸°ì¤€ ì ìˆ˜ (0~3ì )
  â€¢ ë¬¸ì¥ë³„ ë¶„í•  ì²˜ë¦¬ (LSTM + Attention)
  â€¢ ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ (Linear)
  â€¢ ë‘ ëª¨ë¸ì˜ ì•™ìƒë¸”

ğŸ“Š í‰ê°€ ê¸°ì¤€ (11ê°œ):
  1. ë…¼ë¦¬ì„±        2. ì°½ì˜ì„±        3. ì„¤ë“ë ¥
  4. ê·¼ê±°ì˜í’ë¶€í•¨   5. êµ¬ì„±ì˜ì™„ì„±ë„   6. í‘œí˜„ì˜ì •í™•ì„±  
  7. ì–´íœ˜ì˜ë‹¤ì–‘ì„±   8. ë¬¸ë²•ì˜ì •í™•ì„±   9. ë‚´ìš©ì˜ê¹Šì´
  10. ë…ì°½ì„±      11. ì „ì²´ì ì¼ê´€ì„±

ğŸ”§ ì‚¬ìš© ë°©ë²•:
  1. ë°ì´í„° ì¤€ë¹„: CSV íŒŒì¼ (essay_text + 11ê°œ ì ìˆ˜ ì»¬ëŸ¼)
  2. ëª¨ë¸ ì´ˆê¸°í™”: DocumentBertScoringModel()
  3. í•™ìŠµ: model.fit(train_data)
  4. ì˜ˆì¸¡: model.predict_single(essay) ë˜ëŠ” batch_predict_multi()
  5. í‰ê°€: evaluation_multi_regression()

ğŸ’¡ ì£¼ì˜ì‚¬í•­:
  â€¢ ë¬¸ì¥ êµ¬ë¶„ì: "#@ë¬¸ì¥êµ¬ë¶„#" ì‚¬ìš©
  â€¢ ì ìˆ˜ ë²”ìœ„: 0~3ì  (ì‹¤ìˆ˜)
  â€¢ ë°°ì¹˜ í¬ê¸°: GPU ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
  â€¢ í•™ìŠµ ë°ì´í„°: ì¶©ë¶„í•œ ì–‘ í•„ìš” (ìˆ˜ì²œ~ìˆ˜ë§Œ ê°œ)

ğŸ“ˆ ì„±ëŠ¥ ì§€í‘œ:
  â€¢ MSE, MAE, RMSE: íšŒê·€ ì„±ëŠ¥
  â€¢ RÂ², Pearson: ìƒê´€ê´€ê³„
  â€¢ í—ˆìš© ì˜¤ì°¨ ì •í™•ë„: Â±0.1, Â±0.2, Â±0.5
  â€¢ QWK: ìˆœì„œí˜• í‰ê°€ì— ì í•©
    """)
    
    print("\nì‚¬ìš© ì˜ˆì œ ì™„ë£Œ! ğŸ‰")
    print("ì‹¤ì œ ë°ì´í„°ë¡œ í•™ìŠµí•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤.")
