import pandas as pd
import torch
import numpy as np
from model_architecture_bert_multi_scale_multi_loss import DocumentBertScoringModel
import warnings
import os
from datetime import datetime
import time

# ê²½ê³  ë©”ì‹œì§€ ìˆ¨ê¸°ê¸°
warnings.filterwarnings('ignore')

def analyze_data_distribution(train_labels, test_labels, criterion_names):
    """ë°ì´í„° ë¶„í¬ ë¶„ì„"""
    print("\n" + "="*60)
    print("ğŸ“Š ë°ì´í„° ë¶„í¬ ë¶„ì„")
    print("="*60)
    
    train_array = np.array(train_labels)
    test_array = np.array(test_labels)
    
    print(f"í›ˆë ¨ ë°ì´í„° í˜•íƒœ: {train_array.shape}")
    print(f"ê²€ì¦ ë°ì´í„° í˜•íƒœ: {test_array.shape}")
    
    print("\ní‰ê°€ ê¸°ì¤€ë³„ ì ìˆ˜ ë¶„í¬:")
    print("-" * 90)
    print(f"{'í‰ê°€ê¸°ì¤€':<15} {'í›ˆë ¨í‰ê· ':<8} {'í›ˆë ¨í‘œì¤€í¸ì°¨':<10} {'ê²€ì¦í‰ê· ':<8} {'ê²€ì¦í‘œì¤€í¸ì°¨':<10} {'ë‚œì´ë„':<8}")
    print("-" * 90)
    
    difficulty_analysis = []
    
    for i, criterion in enumerate(criterion_names):
        train_mean = np.mean(train_array[:, i])
        train_std = np.std(train_array[:, i])
        test_mean = np.mean(test_array[:, i])
        test_std = np.std(test_array[:, i])
        
        # ë‚œì´ë„ ë¶„ë¥˜
        if train_mean >= 2.5 and train_std <= 0.5:
            difficulty = "ğŸŸ¢ ì‰¬ì›€"
        elif train_mean <= 1.5 or train_std >= 1.2:
            difficulty = "ğŸ”´ ì–´ë ¤ì›€"
        else:
            difficulty = "ğŸŸ¡ ë³´í†µ"
        
        difficulty_analysis.append((criterion, difficulty, train_mean, train_std))
        print(f"{criterion:<15} {train_mean:<8.3f} {train_std:<10.3f} {test_mean:<8.3f} {test_std:<10.3f} {difficulty:<8}")
    
    # ì „ì²´ í†µê³„
    print(f"\nì „ì²´ í†µê³„:")
    print(f"ì „ì²´ í‰ê·  ì ìˆ˜: í›ˆë ¨={np.mean(train_array):.3f}, ê²€ì¦={np.mean(test_array):.3f}")
    print(f"ì „ì²´ í‘œì¤€í¸ì°¨: í›ˆë ¨={np.std(train_array):.3f}, ê²€ì¦={np.std(test_array):.3f}")
    
    # ë¶ˆê· í˜• ì •ë„ ë¶„ì„
    easy_count = sum(1 for _, diff, _, _ in difficulty_analysis if "ì‰¬ì›€" in diff)
    medium_count = sum(1 for _, diff, _, _ in difficulty_analysis if "ë³´í†µ" in diff)
    hard_count = sum(1 for _, diff, _, _ in difficulty_analysis if "ì–´ë ¤ì›€" in diff)
    
    print(f"\nğŸ¯ ë‚œì´ë„ ë¶„í¬:")
    print(f"  ì‰¬ìš´ ê¸°ì¤€: {easy_count}ê°œ")
    print(f"  ë³´í†µ ê¸°ì¤€: {medium_count}ê°œ")
    print(f"  ì–´ë ¤ìš´ ê¸°ì¤€: {hard_count}ê°œ")
    
    if hard_count >= 3:
        print(f"  âš ï¸  ì‹¬ê°í•œ ë¶ˆê· í˜• ê°ì§€! íŠ¹ë³„í•œ í•™ìŠµ ì „ëµì´ í•„ìš”í•©ë‹ˆë‹¤.")
    elif hard_count >= 2:
        print(f"  âš ï¸  ë¶ˆê· í˜• ê°ì§€! ê· í˜• ì¡íŒ í•™ìŠµì„ ì ìš©í•©ë‹ˆë‹¤.")
    else:
        print(f"  âœ… ë¹„êµì  ê· í˜•ì ì¸ ë°ì´í„°ì…ë‹ˆë‹¤.")
    
    return difficulty_analysis

def load_data():
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
    print("ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘...")
    start_time = time.time()
    
    try:
        train = pd.read_csv('./data/train.csv', encoding='utf-8-sig')
        test = pd.read_csv('./data/valid.csv', encoding='utf-8-sig')
    except FileNotFoundError as e:
        print(f"âŒ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print("data/ í´ë”ì— train.csvì™€ valid.csv íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return None, None
    
    print(f"âœ… íŒŒì¼ ë¡œë“œ ì™„ë£Œ: í›ˆë ¨ {len(train)}ê°œ, ê²€ì¦ {len(test)}ê°œ")

    train_essays = train['essay'].to_list()
    train_score_avg = train['essay_score_avg'].to_list()
    test_essays = test['essay'].to_list()
    test_score_avg = test['essay_score_avg'].to_list()

    train_labels = []
    test_labels = []

    # ë¼ë²¨ íŒŒì‹± ë° ê²€ì¦
    print("ğŸ” ë¼ë²¨ ë°ì´í„° íŒŒì‹± ì¤‘...")
    
    for i, score_str in enumerate(train_score_avg):
        try:
            train_scores = score_str.split('#')
            train_scores = [float(score) for score in train_scores]
            if len(train_scores) != 11:
                print(f"âš ï¸  í›ˆë ¨ ìƒ˜í”Œ {i}: ì ìˆ˜ ê°œìˆ˜ {len(train_scores)}ê°œ (ê¸°ëŒ€ê°’: 11)")
            # 0-3 ë²”ìœ„ ê²€ì¦
            train_scores = [max(0.0, min(3.0, score)) for score in train_scores]
            train_labels.append(train_scores)
        except Exception as e:
            print(f"âŒ í›ˆë ¨ ë°ì´í„° {i} íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None, None

    for i, score_str in enumerate(test_score_avg):
        try:
            test_scores = score_str.split('#')
            test_scores = [float(score) for score in test_scores]
            if len(test_scores) != 11:
                print(f"âš ï¸  ê²€ì¦ ìƒ˜í”Œ {i}: ì ìˆ˜ ê°œìˆ˜ {len(test_scores)}ê°œ (ê¸°ëŒ€ê°’: 11)")
            # 0-3 ë²”ìœ„ ê²€ì¦
            test_scores = [max(0.0, min(3.0, score)) for score in test_scores]
            test_labels.append(test_scores)
        except Exception as e:
            print(f"âŒ ê²€ì¦ ë°ì´í„° {i} íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None, None
    
    load_time = time.time() - start_time
    print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {load_time:.2f}ì´ˆ)")
    return (train_essays, train_labels), (test_essays, test_labels)

def create_directories():
    """í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±"""
    directories = ['./models', './logs', './train_valid_loss', './results']
    for directory in directories:
        os.makedirs(directory, exist_ok=True)

def get_optimal_settings(device):
    """GPU ë©”ëª¨ë¦¬ì— ë”°ë¥¸ ìµœì  ì„¤ì •"""
    if device == 'cpu':
        return {
            'batch_size': 4,
            'gradient_accumulation_steps': 1,
            'max_grad_norm': 1.0
        }
    
    total_memory = torch.cuda.get_device_properties(0).total_memory
    gpu_gb = total_memory // 1024**3
    
    if gpu_gb >= 24:  # 24GB+
        return {
            'batch_size': 16,
            'gradient_accumulation_steps': 4,
            'max_grad_norm': 1.0
        }
    elif gpu_gb >= 16:  # 16GB+
        return {
            'batch_size': 12,
            'gradient_accumulation_steps': 3,
            'max_grad_norm': 1.0
        }
    elif gpu_gb >= 12:  # 12GB+
        return {
            'batch_size': 10,
            'gradient_accumulation_steps': 2,
            'max_grad_norm': 1.0
        }
    elif gpu_gb >= 8:   # 8GB+
        return {
            'batch_size': 8,
            'gradient_accumulation_steps': 2,
            'max_grad_norm': 1.0
        }
    else:  # 8GB ë¯¸ë§Œ
        return {
            'batch_size': 6,
            'gradient_accumulation_steps': 1,
            'max_grad_norm': 0.5
        }

def main():
    print("="*80)
    print("ğŸš€ ìˆ˜ì •ëœ í•œêµ­ì–´ ì—ì„¸ì´ ìë™ ì±„ì  ì‹œìŠ¤í…œ (ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘)")
    print("="*80)
    
    start_time = time.time()
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸ”§ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    
    if device == 'cuda':
        gpu_memory = torch.cuda.get_device_properties(0).total_memory // 1024**3
        gpu_name = torch.cuda.get_device_name(0)
        print(f"ğŸ’¾ GPU: {gpu_name} ({gpu_memory}GB)")
        
        # GPU ìµœì í™” ì„¤ì •
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
    
    # í‰ê°€ ê¸°ì¤€ ì •ì˜
    CRITERION_NAMES = [
        'ë¬¸ë²• ì •í™•ë„', 'ë‹¨ì–´ ì„ íƒì˜ ì ì ˆì„±', 'ë¬¸ì¥ í‘œí˜„', 'ë¬¸ë‹¨ ë‚´ êµ¬ì¡°', 'ë¬¸ë‹¨ ê°„ êµ¬ì¡°',
        'êµ¬ì¡°ì˜ ì¼ê´€ì„±', 'ë¶„ëŸ‰ì˜ ì ì ˆì„±', 'ì£¼ì œ ëª…ë£Œì„±', 'ì°½ì˜ì„±', 
        'í”„ë¡¬í”„íŠ¸ ë…í•´ë ¥', 'ì„¤ëª…ì˜ êµ¬ì²´ì„±'
    ]
    
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
    create_directories()
    
    # ë°ì´í„° ë¡œë“œ
    data_result = load_data()
    if data_result is None:
        return
    
    (train_essays, train_labels), (test_essays, test_labels) = data_result
    
    # ë°ì´í„° ë¶„í¬ ë¶„ì„
    difficulty_analysis = analyze_data_distribution(train_labels, test_labels, CRITERION_NAMES)
    
    # ìµœì  ì„¤ì • ê²°ì •
    optimal_settings = get_optimal_settings(device)
    print(f"\nâš™ï¸  ìµœì  ì„¤ì •:")
    print(f"   ë°°ì¹˜ í¬ê¸°: {optimal_settings['batch_size']}")
    print(f"   ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì : {optimal_settings['gradient_accumulation_steps']}")
    print(f"   ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘: {optimal_settings['max_grad_norm']}")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    print(f"\nğŸ¤– ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model_start = time.time()
    
    # ê°œì„ ëœ ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ ìƒì„±
    args = {
        'device': device,
        'batch_size': optimal_settings['batch_size'],
        'gradient_accumulation_steps': optimal_settings['gradient_accumulation_steps'],
        'max_grad_norm': optimal_settings['max_grad_norm'],
        'warmup_ratio': 0.1,
        'label_smoothing': 0.1
    }
    
    class Args:
        def __init__(self, **kwargs):
            for key, value in kwargs.items():
                setattr(self, key, value)
    
    model = DocumentBertScoringModel(load_model=False, args=Args(**args))
    
    model_init_time = time.time() - model_start
    print(f"âœ… ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {model_init_time:.2f}ì´ˆ)")
    
    # ë°ì´í„° ì¤€ë¹„
    df_train_essays = pd.DataFrame({'essay': train_essays})
    df_train_labels = pd.DataFrame(train_labels, columns=CRITERION_NAMES)
    df_test_essays = pd.DataFrame({'essay': test_essays})
    df_test_labels = pd.DataFrame(test_labels, columns=CRITERION_NAMES)

    train_data = (df_train_essays['essay'], df_train_labels)
    test_data = (df_test_essays['essay'], df_test_labels)
    
    # ë¶ˆê· í˜• ë¶„ì„
    model.analyze_data_imbalance(train_labels)
    
    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸ“ í•™ìŠµ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80)
    
    try:
        # ìˆ˜ì •ëœ í•™ìŠµ ì‹¤í–‰
        train_start = time.time()
        
        model.fit(
            train_data, 
            test=test_data, 
            mode=f'fixed_{datetime.now().strftime("%Y%m%d_%H%M%S")}',
            patience=6,  # ì¡°ê¸ˆ ë” ë¹ ë¥¸ early stopping
            log_dir='./logs'
        )
        
        train_time = time.time() - train_start
        print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! (ì´ í•™ìŠµ ì‹œê°„: {train_time/60:.1f}ë¶„)")
        
        # ìµœì¢… í‰ê°€
        print(f"\nğŸ“Š ìµœì¢… ëª¨ë¸ í‰ê°€ ì¤‘...")
        eval_start = time.time()
        
        final_mse, final_mae, (true_labels, pred_labels), final_qwk, final_loss, criterion_mse, criterion_mae, criterion_qwk = model.predict_for_regress(
            test_data, writeflag=True
        )
        
        eval_time = time.time() - eval_start
        
        print(f"\nğŸ¯ ìµœì¢… ì„±ëŠ¥ (í‰ê°€ì‹œê°„: {eval_time:.2f}ì´ˆ):")
        print(f"   ì „ì²´ MSE: {final_mse:.4f}")
        print(f"   ì „ì²´ MAE: {final_mae:.4f}")
        print(f"   ì „ì²´ QWK: {final_qwk:.4f}")
        print(f"   í‰ê°€ ì†ì‹¤: {final_loss:.4f}")
        
        # ë‚œì´ë„ë³„ ì„±ëŠ¥ ë¶„ì„
        print(f"\nğŸ“ˆ ë‚œì´ë„ë³„ ì„±ëŠ¥ ë¶„ì„:")
        
        easy_criteria = [i for i, (_, diff, _, _) in enumerate(difficulty_analysis) if "ì‰¬ì›€" in diff]
        medium_criteria = [i for i, (_, diff, _, _) in enumerate(difficulty_analysis) if "ë³´í†µ" in diff]
        hard_criteria = [i for i, (_, diff, _, _) in enumerate(difficulty_analysis) if "ì–´ë ¤ì›€" in diff]
        
        if easy_criteria:
            easy_qwk = np.mean([criterion_qwk[i] for i in easy_criteria])
            print(f"   ğŸŸ¢ ì‰¬ìš´ ê¸°ì¤€ í‰ê·  QWK: {easy_qwk:.4f}")
        
        if medium_criteria:
            medium_qwk = np.mean([criterion_qwk[i] for i in medium_criteria])
            print(f"   ğŸŸ¡ ë³´í†µ ê¸°ì¤€ í‰ê·  QWK: {medium_qwk:.4f}")
        
        if hard_criteria:
            hard_qwk = np.mean([criterion_qwk[i] for i in hard_criteria])
            print(f"   ğŸ”´ ì–´ë ¤ìš´ ê¸°ì¤€ í‰ê·  QWK: {hard_qwk:.4f}")
            
            print(f"\n   ì–´ë ¤ìš´ ê¸°ì¤€ ìƒì„¸:")
            for i in hard_criteria:
                criterion_name = CRITERION_NAMES[i]
                print(f"     {criterion_name}: MSE={criterion_mse[i]:.4f}, QWK={criterion_qwk[i]:.4f}")
        
        # ì„±ëŠ¥ ê°œì„  ì œì•ˆ
        print(f"\nğŸ’¡ ì„±ëŠ¥ ê°œì„  ì œì•ˆ:")
        if final_qwk < 0.7:
            print("   - ë” ë§ì€ í•™ìŠµ ë°ì´í„° í™•ë³´")
            print("   - ë°ì´í„° ì¦ê°• ê¸°ë²• ì ìš©")
            if hard_criteria:
                print("   - ì–´ë ¤ìš´ ê¸°ì¤€ ì „ìš© ëª¨ë¸ ê³ ë ¤")
        elif final_qwk < 0.8:
            print("   - ì•™ìƒë¸” ëª¨ë¸ ì¶”ê°€")
            print("   - í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
        else:
            print("   - í˜„ì¬ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤! ğŸ‰")
        
        # ìƒ˜í”Œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
        print(f"\nğŸ” ìƒ˜í”Œ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸:")
        if len(test_essays) > 0:
            sample_idx = 0
            sample_essay = test_essays[sample_idx]
            sample_true = test_labels[sample_idx]
            
            prediction_start = time.time()
            result_dict, predictions = model.predict_single(sample_essay)
            prediction_time = time.time() - prediction_start
            
            print(f"   ì˜ˆì¸¡ ì‹œê°„: {prediction_time:.3f}ì´ˆ")
            print(f"   ì—ì„¸ì´ ê¸¸ì´: {len(sample_essay)}ì")
            print(f"   ì—ì„¸ì´ ë¯¸ë¦¬ë³´ê¸°: {sample_essay[:100]}...")
            
            # ì˜¤ì°¨ê°€ í° ê¸°ì¤€ 3ê°œë§Œ ì¶œë ¥
            errors = [(CRITERION_NAMES[i], abs(sample_true[i] - predictions[i]), 
                      sample_true[i], predictions[i]) for i in range(11)]
            errors.sort(key=lambda x: x[1], reverse=True)
            
            print(f"\n   ì˜¤ì°¨ê°€ í° ìƒìœ„ 3ê°œ ê¸°ì¤€:")
            for i, (name, error, true_val, pred_val) in enumerate(errors[:3]):
                print(f"     {i+1}. {name}: ì‹¤ì œ={true_val:.3f}, ì˜ˆì¸¡={pred_val:.3f}, ì˜¤ì°¨={error:.3f}")
        
        # ì „ì²´ ì‹¤í–‰ ì‹œê°„
        total_time = time.time() - start_time
        
        print(f"\nğŸ ì‹œìŠ¤í…œ ì‹¤í–‰ ì™„ë£Œ!")
        print(f"   ì´ ì‹¤í–‰ ì‹œê°„: {total_time/60:.1f}ë¶„")
        print(f"   í•™ìŠµ ì‹œê°„: {train_time/60:.1f}ë¶„ ({train_time/total_time*100:.1f}%)")
        print(f"   í‰ê°€ ì‹œê°„: {eval_time:.1f}ì´ˆ ({eval_time/total_time*100:.1f}%)")
        
        # ì„±ëŠ¥ ì§€í‘œ ìš”ì•½
        print(f"\nğŸ“‹ ìµœì¢… ì„±ëŠ¥ ìš”ì•½:")
        print(f"   ğŸ¯ QWK ì ìˆ˜: {final_qwk:.4f}")
        print(f"   ğŸ“‰ MSE: {final_mse:.4f}")
        print(f"   ğŸ“Š MAE: {final_mae:.4f}")
        
        # ì„±ëŠ¥ ë“±ê¸‰ ë§¤ê¸°ê¸°
        if final_qwk >= 0.85:
            grade = "ğŸ¥‡ ìš°ìˆ˜"
        elif final_qwk >= 0.75:
            grade = "ğŸ¥ˆ ì–‘í˜¸"
        elif final_qwk >= 0.65:
            grade = "ğŸ¥‰ ë³´í†µ"
        else:
            grade = "ğŸ“ˆ ê°œì„ í•„ìš”"
        
        print(f"   ë“±ê¸‰: {grade}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (CUDA)
        if device == 'cuda':
            memory_used = torch.cuda.max_memory_allocated() / 1024**3
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   ğŸ’¾ GPU ë©”ëª¨ë¦¬: {memory_used:.1f}GB / {memory_total:.1f}GB ({memory_used/memory_total*100:.1f}%)")
        
        # íŒŒì¼ ì €ì¥ ìœ„ì¹˜
        print(f"\nğŸ“„ ìƒì„±ëœ íŒŒì¼:")
        print(f"   ğŸ’¾ ëª¨ë¸: ./models/")
        print(f"   ğŸ“Š ë¡œê·¸: ./logs/")
        print(f"   ğŸ“ˆ í•™ìŠµê³¡ì„ : ./train_valid_loss/")
        print(f"   ğŸ“ ì˜ˆì¸¡ê²°ê³¼: ./models/result.txt")
        
        # ë‹¤ìŒ ë‹¨ê³„ ì œì•ˆ
        print(f"\nğŸ”œ ë‹¤ìŒ ë‹¨ê³„:")
        print("   1. ì €ì¥ëœ ëª¨ë¸ë¡œ ìƒˆë¡œìš´ ì—ì„¸ì´ ì±„ì ")
        print("   2. í•™ìŠµ ê³¡ì„  ì‹œê°í™”ë¡œ ì„±ëŠ¥ ë¶„ì„")
        print("   3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ì„±ëŠ¥ ê°œì„ ")
        print("   4. ë” ë§ì€ ë°ì´í„°ë¡œ ì¬í•™ìŠµ")
        
        print(f"\nì‚¬ìš© ì˜ˆì‹œ:")
        print("   # ìƒˆë¡œìš´ ì—ì„¸ì´ ì±„ì ")
        print("   model.load_best_model()")
        print("   result, scores = model.predict_single('ìƒˆë¡œìš´ ì—ì„¸ì´ ë‚´ìš©...')")
        print("   print(result)")
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ í•™ìŠµì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        interrupted_time = time.time() - start_time
        print(f"   ë¶€ë¶„ ì‹¤í–‰ ì‹œê°„: {interrupted_time/60:.1f}ë¶„")
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
        # ì˜¤ë¥˜ í•´ê²° ì œì•ˆ
        error_str = str(e)
        print(f"\nğŸ”§ ì˜¤ë¥˜ í•´ê²° ì œì•ˆ:")
        
        if "CUDA out of memory" in error_str:
            print("   - ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”: batch_sizeë¥¼ 4 ë˜ëŠ” 2ë¡œ ì„¤ì •")
            print("   - ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ëŠ˜ë ¤ë³´ì„¸ìš”: gradient_accumulation_steps ì¦ê°€")
            print("   - ë¬¸ì„œ ê¸¸ì´ë¥¼ ì¤„ì—¬ë³´ì„¸ìš”: max_doc_lengthë¥¼ 256ìœ¼ë¡œ ì„¤ì •")
        elif "expected Tensor" in error_str:
            print("   - ì´ë¯¸ ìˆ˜ì •ëœ ë²„ì „ì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤")
            print("   - ë°ì´í„° í˜•íƒœë¥¼ í™•ì¸í•´ë³´ì„¸ìš”")
        elif "FileNotFoundError" in error_str:
            print("   - data/ í´ë”ì— train.csv, valid.csv íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸")
            print("   - íŒŒì¼ ê²½ë¡œì™€ ì¸ì½”ë”©ì„ í™•ì¸í•´ë³´ì„¸ìš”")
        else:
            print("   - GitHub ì´ìŠˆë¥¼ í™•ì¸í•˜ê±°ë‚˜ ë¬¸ì˜í•´ì£¼ì„¸ìš”")
            print("   - ë¡œê·¸ íŒŒì¼ì„ ì²¨ë¶€í•´ì£¼ì‹œë©´ ë„ì›€ì´ ë©ë‹ˆë‹¤")
    
    finally:
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        if device == 'cuda':
            torch.cuda.empty_cache()
            print("ğŸ§¹ GPU ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        
        end_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"\nâ° ì‹œìŠ¤í…œ ì¢…ë£Œ: {end_time}")
        print("="*80)

def quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (ì†ŒëŸ‰ ë°ì´í„°ë¡œ ë™ì‘ í™•ì¸)"""
    print("ğŸš€ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ")
    print("="*50)
    
    # ê°€ìƒ ë°ì´í„° ìƒì„±
    test_essays = [
        "í™˜ê²½ ë³´í˜¸ëŠ” ì¤‘ìš”í•©ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ìš°ë¦¬ ëª¨ë‘ ë…¸ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ì¬í™œìš©ì„ ì‹¤ì²œí•©ì‹œë‹¤.",
        "êµìœ¡ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ì¢‹ì€ êµìœ¡ì´ ë¯¸ë˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ëª¨ë“  í•™ìƒì´ ê³µí‰í•œ ê¸°íšŒë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤.",
        "ê¸°ìˆ  ë°œì „ì´ ì‚¬íšŒì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#ì¸ê³µì§€ëŠ¥ì´ ìš°ë¦¬ ìƒí™œì„ ë°”ê¾¸ê³  ìˆìŠµë‹ˆë‹¤.#@ë¬¸ì¥êµ¬ë¶„#í•˜ì§€ë§Œ ë¶€ì‘ìš©ë„ ìˆìŠµë‹ˆë‹¤."
    ]
    
    test_labels = [
        [2.5, 2.0, 1.5, 2.2, 1.8, 2.1, 2.3, 2.4, 2.0, 1.6, 2.2],
        [2.8, 2.5, 2.0, 2.6, 2.2, 2.4, 2.7, 2.8, 2.3, 2.1, 2.5],
        [2.3, 2.1, 1.8, 2.0, 1.9, 2.0, 2.2, 2.3, 2.4, 1.9, 2.1]
    ]
    
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_essays)}ê°œ ì—ì„¸ì´")
    
    # ê°„ë‹¨í•œ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    args = {
        'device': device,
        'batch_size': 2,
        'gradient_accumulation_steps': 1,
        'max_grad_norm': 1.0
    }
    
    class Args:
        def __init__(self, **kwargs):
            for key, value in kwargs.items():
                setattr(self, key, value)
    
    model = DocumentBertScoringModel(load_model=False, args=Args(**args))
    
    # ë‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸
    print("\në‹¨ì¼ ì˜ˆì¸¡ í…ŒìŠ¤íŠ¸:")
    result_dict, predictions = model.predict_single(test_essays[0])
    
    print("ì˜ˆì¸¡ ê²°ê³¼:")
    for criterion, score in result_dict.items():
        print(f"  {criterion}: {score:.3f}")
    
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ! ëª¨ë¸ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")

if __name__ == "__main__":
    import sys
    
    # ì»¤ë§¨ë“œë¼ì¸ ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1:
        if sys.argv[1] == 'test':
            quick_test()
        elif sys.argv[1] == 'help':
            print("ì‚¬ìš©ë²•:")
            print("  python main_fixed.py        # ì „ì²´ í•™ìŠµ ì‹¤í–‰")
            print("  python main_fixed.py test   # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸")
            print("  python main_fixed.py help   # ë„ì›€ë§")
        else:
            print("ì•Œ ìˆ˜ ì—†ëŠ” ì˜µì…˜ì…ë‹ˆë‹¤. 'help'ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.")
    else:
        main()