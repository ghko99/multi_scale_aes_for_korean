from scipy.stats import pearsonr
import numpy as np
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns


def quadratic_weighted_kappa_single(rater_a, rater_b, min_rating=None, max_rating=None):
    """ë‹¨ì¼ ê¸°ì¤€ì— ëŒ€í•œ QWK ê³„ì‚°"""
    rater_a = np.array(rater_a, dtype=int)
    rater_b = np.array(rater_b, dtype=int)
    
    if min_rating is None:
        min_rating = min(min(rater_a), min(rater_b))
    if max_rating is None:
        max_rating = max(max(rater_a), max(rater_b))
    
    # í˜¼ë™ í–‰ë ¬ ìƒì„±
    num_ratings = max_rating - min_rating + 1
    conf_mat = np.zeros((num_ratings, num_ratings))
    
    for a, b in zip(rater_a, rater_b):
        conf_mat[a - min_rating][b - min_rating] += 1
    
    # ê°€ì¤‘ì¹˜ í–‰ë ¬ ìƒì„±
    weights = np.zeros((num_ratings, num_ratings))
    for i in range(num_ratings):
        for j in range(num_ratings):
            weights[i, j] = ((i - j) ** 2) / ((num_ratings - 1) ** 2)
    
    # ê¸°ëŒ€ í–‰ë ¬ ê³„ì‚°
    hist_a = np.sum(conf_mat, axis=1)
    hist_b = np.sum(conf_mat, axis=0)
    expected = np.outer(hist_a, hist_b) / len(rater_a)
    
    # QWK ê³„ì‚°
    numerator = np.sum(weights * conf_mat)
    denominator = np.sum(weights * expected)
    
    if denominator == 0:
        return 1.0
    
    return 1.0 - numerator / denominator



# ê¸°ì¡´ í•¨ìˆ˜ì™€ì˜ í˜¸í™˜ì„±ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜ë“¤
def quadratic_weighted_kappa_multi(rater_a, rater_b):
    """ë©€í‹° íšŒê·€ë¥¼ ìœ„í•œ QWK (ê° ê¸°ì¤€ë³„ë¡œ ê³„ì‚° í›„ í‰ê· )"""
    rater_a = np.array(rater_a)
    rater_b = np.array(rater_b)
    
    if len(rater_a.shape) == 1:
        return quadratic_weighted_kappa_single(rater_a, rater_b)
    
    qwk_scores = []
    for i in range(rater_a.shape[1]):
        # 0-3 ë²”ìœ„ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜ (QWK ê³„ì‚°ì„ ìœ„í•´)
        a_rounded = np.round(rater_a[:, i] * 3).astype(int)  # 0-3 ì •ìˆ˜
        b_rounded = np.round(rater_b[:, i] * 3).astype(int)  # 0-3 ì •ìˆ˜
        
        qwk = quadratic_weighted_kappa_single(a_rounded, b_rounded, min_rating=0, max_rating=3)
        qwk_scores.append(qwk)
    
    return np.mean(qwk_scores)



def evaluation_multi_regression(true_labels, pred_labels, criterion_names=None):
    """
    11ê°œ í‰ê°€ ê¸°ì¤€ì— ëŒ€í•œ ë©€í‹°-íƒœìŠ¤í¬ íšŒê·€ í‰ê°€ í•¨ìˆ˜
    
    Args:
        true_labels: (N, 11) ì‹¤ì œ ë¼ë²¨
        pred_labels: (N, 11) ì˜ˆì¸¡ ë¼ë²¨
        criterion_names: 11ê°œ í‰ê°€ ê¸°ì¤€ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        results: í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    true_labels = np.array(true_labels)
    pred_labels = np.array(pred_labels)
    
    assert true_labels.shape == pred_labels.shape
    assert true_labels.shape[1] == 11, "11ê°œ í‰ê°€ ê¸°ì¤€ì´ì–´ì•¼ í•©ë‹ˆë‹¤"
    
    if criterion_names is None:
        criterion_names = [f'ê¸°ì¤€_{i+1}' for i in range(11)]
    
    results = {
        'overall': {},
        'by_criterion': {},
        'correlation_matrix': None,
        'score_distribution': {}
    }
    
    # ì „ì²´ ì„±ëŠ¥ í‰ê°€
    overall_mse = mean_squared_error(true_labels.flatten(), pred_labels.flatten())
    overall_mae = mean_absolute_error(true_labels.flatten(), pred_labels.flatten())
    overall_rmse = np.sqrt(overall_mse)
    overall_r2 = r2_score(true_labels.flatten(), pred_labels.flatten())
    overall_pearson = pearsonr(true_labels.flatten(), pred_labels.flatten())[0]
    overall_qwk = quadratic_weighted_kappa_multi(np.round(true_labels).astype(int).flatten() , np.round(pred_labels).astype(int).flatten())

    results['overall'] = {
        'MSE': overall_mse,
        'MAE': overall_mae,
        'RMSE': overall_rmse,
        'R2': overall_r2,
        'Pearson': overall_pearson,
        "QWK" : overall_qwk
    }
    
    # í‰ê°€ ê¸°ì¤€ë³„ ì„±ëŠ¥ í‰ê°€
    criterion_results = {}
    for i, criterion in enumerate(criterion_names):
        true_criterion = true_labels[:, i]
        pred_criterion = pred_labels[:, i]
        
        mse = mean_squared_error(true_criterion, pred_criterion)
        mae = mean_absolute_error(true_criterion, pred_criterion)
        rmse = np.sqrt(mse)
        r2 = r2_score(true_criterion, pred_criterion)
        pearson = pearsonr(true_criterion, pred_criterion)[0]
        qwk = quadratic_weighted_kappa_multi(np.round(true_criterion).astype(int), np.round(pred_criterion).astype(int))


        # ì •í™•ë„ (í—ˆìš© ì˜¤ì°¨ ë‚´)
        tolerance_01_acc = np.mean(np.abs(pred_criterion - true_criterion) <= 0.1)
        tolerance_02_acc = np.mean(np.abs(pred_criterion - true_criterion) <= 0.2)
        tolerance_05_acc = np.mean(np.abs(pred_criterion - true_criterion) <= 0.5)
        
        criterion_results[criterion] = {
            'MSE': mse,
            'MAE': mae,
            'RMSE': rmse,
            'R2': r2,
            'Pearson': pearson,
            "QWK" : qwk,
            'Tolerance_0.1_Acc': tolerance_01_acc,
            'Tolerance_0.2_Acc': tolerance_02_acc,
            'Tolerance_0.5_Acc': tolerance_05_acc
        }
    
    results['by_criterion'] = criterion_results
    
    # í‰ê°€ ê¸°ì¤€ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„
    true_corr_matrix = np.corrcoef(true_labels.T)
    pred_corr_matrix = np.corrcoef(pred_labels.T)
    
    results['correlation_matrix'] = {
        'true_labels': true_corr_matrix,
        'pred_labels': pred_corr_matrix,
        'difference': np.abs(true_corr_matrix - pred_corr_matrix)
    }
    
    # ì ìˆ˜ ë¶„í¬ ë¶„ì„
    for i, criterion in enumerate(criterion_names):
        results['score_distribution'][criterion] = {
            'true_mean': np.mean(true_labels[:, i]),
            'true_std': np.std(true_labels[:, i]),
            'pred_mean': np.mean(pred_labels[:, i]),
            'pred_std': np.std(pred_labels[:, i]),
            'true_range': (np.min(true_labels[:, i]), np.max(true_labels[:, i])),
            'pred_range': (np.min(pred_labels[:, i]), np.max(pred_labels[:, i]))
        }
    
    return results


def print_evaluation_results(results, detailed=True):
    """í‰ê°€ ê²°ê³¼ë¥¼ ë³´ê¸° ì¢‹ê²Œ ì¶œë ¥"""
    print("=" * 60)
    print("ë©€í‹°-íƒœìŠ¤í¬ íšŒê·€ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼")
    print("=" * 60)
    
    # ì „ì²´ ì„±ëŠ¥
    print("\nğŸ” ì „ì²´ ì„±ëŠ¥:")
    overall = results['overall']
    print(f"  MSE: {overall['MSE']:.4f}")
    print(f"  MAE: {overall['MAE']:.4f}")
    print(f"  RMSE: {overall['RMSE']:.4f}")
    print(f"  RÂ²: {overall['R2']:.4f}")
    print(f"  Pearson ìƒê´€ê³„ìˆ˜: {overall['Pearson']:.4f}")
    print(f"  Quadratic Weighted Kappa Score: {overall['QWK']:.4f}")
    
    # í‰ê°€ ê¸°ì¤€ë³„ ì„±ëŠ¥
    print("\nğŸ“Š í‰ê°€ ê¸°ì¤€ë³„ ì„±ëŠ¥:")
    print("-" * 100)
    print(f"{'í‰ê°€ê¸°ì¤€':<12} {'MSE':<8} {'MAE':<8} {'RMSE':<8} {'RÂ²':<8} {'Pearson':<8} {'QWK':<8} {'Â±0.1ì •í™•ë„':<10} {'Â±0.2ì •í™•ë„':<10} {'Â±0.5ì •í™•ë„':<10}")
    print("-" * 100)
    
    for criterion, metrics in results['by_criterion'].items():
        print(f"{criterion:<12} {metrics['MSE']:<8.4f} {metrics['MAE']:<8.4f} {metrics['RMSE']:<8.4f} "
              f"{metrics['R2']:<8.4f} {metrics['Pearson']:<8.4f} {metrics['QWK']:<8.4f} "
              f"{metrics['Tolerance_0.1_Acc']:<10.4f} {metrics['Tolerance_0.2_Acc']:<10.4f} {metrics['Tolerance_0.5_Acc']:<10.4f}")
    
    if detailed:
        # ì ìˆ˜ ë¶„í¬ ë¶„ì„
        print("\nğŸ“ˆ ì ìˆ˜ ë¶„í¬ ë¶„ì„:")
        print("-" * 80)
        print(f"{'í‰ê°€ê¸°ì¤€':<12} {'ì‹¤ì œí‰ê· ':<10} {'ì˜ˆì¸¡í‰ê· ':<10} {'ì‹¤ì œí‘œì¤€í¸ì°¨':<12} {'ì˜ˆì¸¡í‘œì¤€í¸ì°¨':<12}")
        print("-" * 80)
        
        for criterion, dist in results['score_distribution'].items():
            print(f"{criterion:<12} {dist['true_mean']:<10.3f} {dist['pred_mean']:<10.3f} "
                  f"{dist['true_std']:<12.3f} {dist['pred_std']:<12.3f}")


def plot_evaluation_results(results, save_path=None):
    """í‰ê°€ ê²°ê³¼ë¥¼ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    
    # 1. í‰ê°€ ê¸°ì¤€ë³„ MSE ë¹„êµ
    criteria = list(results['by_criterion'].keys())
    mse_values = [results['by_criterion'][c]['MSE'] for c in criteria]
    
    axes[0, 0].bar(range(len(criteria)), mse_values, color='skyblue')
    axes[0, 0].set_title('í‰ê°€ ê¸°ì¤€ë³„ MSE')
    axes[0, 0].set_xlabel('í‰ê°€ ê¸°ì¤€')
    axes[0, 0].set_ylabel('MSE')
    axes[0, 0].set_xticks(range(len(criteria)))
    axes[0, 0].set_xticklabels(criteria, rotation=45, ha='right')
    
    # 2. í‰ê°€ ê¸°ì¤€ë³„ Pearson ìƒê´€ê³„ìˆ˜
    pearson_values = [results['by_criterion'][c]['Pearson'] for c in criteria]
    
    axes[0, 1].bar(range(len(criteria)), pearson_values, color='lightcoral')
    axes[0, 1].set_title('í‰ê°€ ê¸°ì¤€ë³„ Pearson ìƒê´€ê³„ìˆ˜')
    axes[0, 1].set_xlabel('í‰ê°€ ê¸°ì¤€')
    axes[0, 1].set_ylabel('Pearson ìƒê´€ê³„ìˆ˜')
    axes[0, 1].set_xticks(range(len(criteria)))
    axes[0, 1].set_xticklabels(criteria, rotation=45, ha='right')
    axes[0, 1].set_ylim(0, 1)

    # 2. í‰ê°€ ê¸°ì¤€ë³„ QWK
    pearson_values = [results['by_criterion'][c]['QWK'] for c in criteria]
    
    axes[0, 1].bar(range(len(criteria)), pearson_values, color='lightcoral')
    axes[0, 1].set_title('í‰ê°€ ê¸°ì¤€ë³„ QWK')
    axes[0, 1].set_xlabel('í‰ê°€ ê¸°ì¤€')
    axes[0, 1].set_ylabel('QWK')
    axes[0, 1].set_xticks(range(len(criteria)))
    axes[0, 1].set_xticklabels(criteria, rotation=45, ha='right')
    axes[0, 1].set_ylim(0, 1)
    
    # 3. ì‹¤ì œ ë¼ë²¨ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    im1 = axes[1, 0].imshow(results['correlation_matrix']['true_labels'], 
                           cmap='coolwarm', vmin=-1, vmax=1)
    axes[1, 0].set_title('ì‹¤ì œ ë¼ë²¨ ê°„ ìƒê´€ê´€ê³„')
    axes[1, 0].set_xticks(range(len(criteria)))
    axes[1, 0].set_yticks(range(len(criteria)))
    axes[1, 0].set_xticklabels(criteria, rotation=45, ha='right')
    axes[1, 0].set_yticklabels(criteria)
    plt.colorbar(im1, ax=axes[1, 0])
    
    # 4. ì˜ˆì¸¡ ë¼ë²¨ ìƒê´€ê´€ê³„ íˆíŠ¸ë§µ
    im2 = axes[1, 1].imshow(results['correlation_matrix']['pred_labels'], 
                           cmap='coolwarm', vmin=-1, vmax=1)
    axes[1, 1].set_title('ì˜ˆì¸¡ ë¼ë²¨ ê°„ ìƒê´€ê´€ê³„')
    axes[1, 1].set_xticks(range(len(criteria)))
    axes[1, 1].set_yticks(range(len(criteria)))
    axes[1, 1].set_xticklabels(criteria, rotation=45, ha='right')
    axes[1, 1].set_yticklabels(criteria)
    plt.colorbar(im2, ax=axes[1, 1])
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()


def analyze_prediction_errors(true_labels, pred_labels, criterion_names=None, top_k=5):
    """ì˜ˆì¸¡ ì˜¤ì°¨ ìƒì„¸ ë¶„ì„"""
    true_labels = np.array(true_labels)
    pred_labels = np.array(pred_labels)
    
    if criterion_names is None:
        criterion_names = [f'ê¸°ì¤€_{i+1}' for i in range(11)]
    
    errors = np.abs(true_labels - pred_labels)
    
    print("\nğŸ” ì˜ˆì¸¡ ì˜¤ì°¨ ìƒì„¸ ë¶„ì„:")
    print("=" * 50)
    
    # ê°€ì¥ í° ì˜¤ì°¨ë¥¼ ë³´ì´ëŠ” ìƒ˜í”Œë“¤
    max_errors_per_sample = np.max(errors, axis=1)
    worst_samples = np.argsort(max_errors_per_sample)[-top_k:]
    
    print(f"\nê°€ì¥ í° ì˜¤ì°¨ë¥¼ ë³´ì´ëŠ” ìƒìœ„ {top_k}ê°œ ìƒ˜í”Œ:")
    for i, sample_idx in enumerate(reversed(worst_samples)):
        max_error = max_errors_per_sample[sample_idx]
        worst_criterion_idx = np.argmax(errors[sample_idx])
        worst_criterion = criterion_names[worst_criterion_idx]
        
        print(f"  {i+1}. ìƒ˜í”Œ {sample_idx}: ìµœëŒ€ ì˜¤ì°¨ {max_error:.3f} ({worst_criterion})")
        print(f"     ì‹¤ì œ: {true_labels[sample_idx, worst_criterion_idx]:.3f}, "
              f"ì˜ˆì¸¡: {pred_labels[sample_idx, worst_criterion_idx]:.3f}")
    
    # ê°€ì¥ ì–´ë ¤ìš´ í‰ê°€ ê¸°ì¤€ë“¤
    mean_errors_per_criterion = np.mean(errors, axis=0)
    difficult_criteria = np.argsort(mean_errors_per_criterion)[-top_k:]
    
    print(f"\nê°€ì¥ ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ ìƒìœ„ {top_k}ê°œ í‰ê°€ ê¸°ì¤€:")
    for i, criterion_idx in enumerate(reversed(difficult_criteria)):
        criterion = criterion_names[criterion_idx]
        mean_error = mean_errors_per_criterion[criterion_idx]
        print(f"  {i+1}. {criterion}: í‰ê·  ì˜¤ì°¨ {mean_error:.3f}")




