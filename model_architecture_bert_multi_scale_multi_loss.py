import os
import torch
from transformers import BertConfig, CONFIG_NAME, AutoTokenizer
from kobert_transformers import get_tokenizer
from document_bert_architectures import DocumentBertCombineWordDocumentLinear, DocumentBertSentenceChunkAttentionLSTM
from evaluate import quadratic_weighted_kappa_multi
from encoder import encode_documents_by_sentence, encode_documents_full_text
from torch.nn import functional as F
import numpy as np
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, mean_absolute_error
from datetime import datetime
import time
import warnings

# ê°œì„ ëœ ì†ì‹¤ í•¨ìˆ˜ë“¤
class AdaptiveLossWeighting(torch.nn.Module):
    """ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ ì¡°ì •"""
    def __init__(self, num_tasks=3, init_weights=None):
        super().__init__()
        if init_weights is None:
            init_weights = [1.0, 0.5, 0.3]  # MSE, Sim, MR ì´ˆê¸° ê°€ì¤‘ì¹˜
        self.log_weights = torch.nn.Parameter(torch.log(torch.tensor(init_weights)))
    
    def forward(self, losses):
        weights = torch.exp(self.log_weights)
        return torch.sum(weights * losses), weights

def improved_sim_loss(y, yhat, temperature=0.1):
    """ê°œì„ ëœ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì†ì‹¤ (ì˜¨ë„ ë§¤ê°œë³€ìˆ˜ ì¶”ê°€)"""
    cos = F.cosine_similarity(y, yhat, dim=1)
    # ì˜¨ë„ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ì—¬ ë” ë¯¼ê°í•˜ê²Œ ë§Œë“¤ê¸°
    loss = torch.mean((1 - cos) / temperature)
    return loss

def focal_mse_loss(pred, target, alpha=0.25, gamma=2.0):
    """Focal MSE Loss - ì–´ë ¤ìš´ ìƒ˜í”Œì— ë” ì§‘ì¤‘"""
    mse = F.mse_loss(pred, target, reduction='none')
    # í‰ê·  MSEë¡œ ì •ê·œí™”
    normalized_mse = mse / (torch.mean(mse) + 1e-8)
    # Focal weight ê³„ì‚°
    focal_weight = alpha * torch.pow(normalized_mse, gamma)
    return torch.mean(focal_weight * mse)

def improved_mr_loss_func(pred, label, margin=0.1):
    """ê°œì„ ëœ ìˆœìœ„ ì†ì‹¤ (ë§ˆì§„ ì¶”ê°€) - íƒ€ì… ì•ˆì „ì„± ë³´ì¥"""
    device = pred.device
    
    if label.size(0) <= 1:
        return torch.tensor(0.0, device=device, dtype=pred.dtype)
    
    total_mr_loss = torch.tensor(0.0, device=device, dtype=pred.dtype)
    num_criteria = pred.size(1)
    
    for criterion in range(num_criteria):
        pred_criterion = pred[:, criterion]
        label_criterion = label[:, criterion]
        
        mr_loss = torch.tensor(0.0, device=device, dtype=pred.dtype)
        pairs = 0
        
        for i in range(label.size(0)):
            for j in range(i + 1, label.size(0)):
                # ì‹¤ì œ ìˆœìœ„ ì°¨ì´
                true_diff = label_criterion[i] - label_criterion[j]
                pred_diff = pred_criterion[i] - pred_criterion[j]
                
                # ìˆœìœ„ê°€ ë’¤ë°”ë€Œì—ˆê³  ì°¨ì´ê°€ ìœ ì˜ë¯¸í•œ ê²½ìš°ì—ë§Œ íŒ¨ë„í‹°
                if torch.abs(true_diff) > 0.1:  # ìµœì†Œ ì°¨ì´ ì„ê³„ê°’
                    if true_diff * pred_diff < 0:  # ë¶€í˜¸ê°€ ë‹¤ë¦„
                        penalty = torch.clamp(margin - pred_diff * torch.sign(true_diff), min=0)
                        mr_loss += penalty
                        pairs += 1
        
        if pairs > 0:
            total_mr_loss += mr_loss / pairs
    
    # í‰ê·  ê³„ì‚° - í•­ìƒ í…ì„œ ë°˜í™˜ ë³´ì¥
    if num_criteria > 0:
        result = total_mr_loss / num_criteria
    else:
        result = torch.tensor(0.0, device=device, dtype=pred.dtype)
    
    return result

# ê· í˜• ì¡íŒ ì†ì‹¤ í•¨ìˆ˜ (ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘)
def balanced_multi_criterion_loss(pred, target, criterion_weights=None):
    """í‰ê°€ ê¸°ì¤€ë³„ ê· í˜•ì„ ê³ ë ¤í•œ ì†ì‹¤ í•¨ìˆ˜"""
    device = pred.device
    
    if criterion_weights is None:
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ì–´ë ¤ìš´ ê¸°ì¤€ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜)
        # ì¸ë±ìŠ¤: ë¬¸ë²•ì •í™•ë„, ë‹¨ì–´ì„ íƒ, ë¬¸ì¥í‘œí˜„, ë¬¸ë‹¨ë‚´êµ¬ì¡°, ë¬¸ë‹¨ê°„êµ¬ì¡°, êµ¬ì¡°ì¼ê´€ì„±, ë¶„ëŸ‰ì ì ˆì„±, ì£¼ì œëª…ë£Œì„±, ì°½ì˜ì„±, í”„ë¡¬í”„íŠ¸ë…í•´ë ¥, ì„¤ëª…êµ¬ì²´ì„±
        weights = torch.tensor([
            0.8,  # ë¬¸ë²• ì •í™•ë„ (ì‰¬ì›€)
            0.8,  # ë‹¨ì–´ ì„ íƒì˜ ì ì ˆì„± (ì‰¬ì›€)
            1.5,  # ë¬¸ì¥ í‘œí˜„ (ì–´ë ¤ì›€)
            0.9,  # ë¬¸ë‹¨ ë‚´ êµ¬ì¡° (ë³´í†µ)
            1.5,  # ë¬¸ë‹¨ ê°„ êµ¬ì¡° (ì–´ë ¤ì›€)
            1.2,  # êµ¬ì¡°ì˜ ì¼ê´€ì„± (ë³´í†µ-ì–´ë ¤ì›€)
            0.8,  # ë¶„ëŸ‰ì˜ ì ì ˆì„± (ì‰¬ì›€)
            0.8,  # ì£¼ì œ ëª…ë£Œì„± (ì‰¬ì›€)
            1.1,  # ì°½ì˜ì„± (ë³´í†µ)
            1.5,  # í”„ë¡¬í”„íŠ¸ ë…í•´ë ¥ (ì–´ë ¤ì›€)
            0.9   # ì„¤ëª…ì˜ êµ¬ì²´ì„± (ë³´í†µ)
        ], device=device, dtype=pred.dtype)
    else:
        weights = torch.tensor(criterion_weights, device=device, dtype=pred.dtype)
    
    # ê¸°ì¤€ë³„ MSE ê³„ì‚°
    criterion_losses = []
    for i in range(pred.size(1)):
        criterion_mse = F.mse_loss(pred[:, i], target[:, i])
        weighted_mse = criterion_mse * weights[i]
        criterion_losses.append(weighted_mse)
    
    return torch.stack(criterion_losses).mean()

class DocumentBertScoringModel():
    def __init__(self, load_model=False, chunk_model_path=None, word_doc_model_path=None, config=None, args=None):
        if args is not None:
            self.args = vars(args)
        else:
            # ê°œì„ ëœ ê¸°ë³¸ ì„¤ì •
            self.args = {
                'device': 'cuda' if torch.cuda.is_available() else 'cpu',
                'batch_size': 8,  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
                'model_directory': './models',
                'result_file': 'result.txt',
                'gradient_accumulation_steps': 2,  # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
                'max_grad_norm': 1.0,
                'warmup_ratio': 0.1,
                'label_smoothing': 0.1
            }
            
        # í•œêµ­ì–´ BERT í† í¬ë‚˜ì´ì € ì‚¬ìš©
        self.bert_tokenizer = get_tokenizer()
        
        # config ì„¤ì •
        if config is None:
            from transformers import AutoConfig
            config = AutoConfig.from_pretrained("monologg/kobert")
        self.config = config
        
        # ë¬¸ì¥ë³„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„¤ì •
        self.max_sentence_length = 128
        self.max_doc_length = 512
        
        # 11ê°œ í‰ê°€ ê¸°ì¤€ ì´ë¦„
        self.criterion_names = [
            'ë¬¸ë²• ì •í™•ë„', 'ë‹¨ì–´ ì„ íƒì˜ ì ì ˆì„±', 'ë¬¸ì¥ í‘œí˜„', 'ë¬¸ë‹¨ ë‚´ êµ¬ì¡°', 'ë¬¸ë‹¨ ê°„ êµ¬ì¡°',
            'êµ¬ì¡°ì˜ ì¼ê´€ì„±', 'ë¶„ëŸ‰ì˜ ì ì ˆì„±', 'ì£¼ì œ ëª…ë£Œì„±', 'ì°½ì˜ì„±', 
            'í”„ë¡¬í”„íŠ¸ ë…í•´ë ¥', 'ì„¤ëª…ì˜ êµ¬ì²´ì„±'
        ]
        
        # ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜
        self.adaptive_loss = AdaptiveLossWeighting().to(self.args['device'])
        
        print(f"Device: {self.args['device']}")
        print(f"Max sentence length: {self.max_sentence_length}")
        print(f"Max document length: {self.max_doc_length}")
        print(f"í‰ê°€ ê¸°ì¤€: {self.criterion_names}")
        print(f"ë°°ì¹˜ í¬ê¸°: {self.args['batch_size']}")
        print(f"ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…: {self.args['gradient_accumulation_steps']}")
        
        # ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ì´ˆê¸°í™”
        if load_model and chunk_model_path and word_doc_model_path:
            self.bert_regression_by_word_document = DocumentBertCombineWordDocumentLinear.from_pretrained(
                word_doc_model_path, config=config)
            self.bert_regression_by_chunk = DocumentBertSentenceChunkAttentionLSTM.from_pretrained(
                chunk_model_path, config=config)
        else:
            self.bert_regression_by_word_document = DocumentBertCombineWordDocumentLinear(bert_model_config=self.config)
            self.bert_regression_by_chunk = DocumentBertSentenceChunkAttentionLSTM(bert_model_config=self.config)

    def predict_for_regress(self, data, writeflag=False):
        """11ê°œ í‰ê°€ ê¸°ì¤€ì— ëŒ€í•œ ë©€í‹°-íƒœìŠ¤í¬ íšŒê·€ ì˜ˆì¸¡"""
        correct_output = None
        
        if isinstance(data, tuple) and len(data) == 2:
            # ë¬¸ì¥ë³„ ë¶„í•  ì¸ì½”ë”©
            document_representations_sentence, document_sequence_lengths_sentence = encode_documents_by_sentence(
                data[0], self.bert_tokenizer, max_input_length=self.max_sentence_length)
            
            # ì „ì²´ ë¬¸ì„œ ì¸ì½”ë”©
            document_representations_full, document_sequence_lengths_full = encode_documents_full_text(
                data[0], self.bert_tokenizer, max_input_length=self.max_doc_length)
            
            # ë¼ë²¨ ì²˜ë¦¬
            if isinstance(data[1], (list, np.ndarray)):
                if len(np.array(data[1]).shape) == 1:
                    labels_array = np.array(data[1])
                    if labels_array.shape[0] % 11 == 0:
                        correct_output = torch.FloatTensor(labels_array.reshape(-1, 11))
                    else:
                        raise ValueError("ë¼ë²¨ ë°ì´í„° í˜•íƒœê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (N, 11) í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤.")
                else:
                    correct_output = torch.FloatTensor(data[1])
            else:
                correct_output = torch.FloatTensor(data[1])

        # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.bert_regression_by_word_document.to(device=self.args['device'])
        self.bert_regression_by_chunk.to(device=self.args['device'])

        self.bert_regression_by_word_document.eval()
        self.bert_regression_by_chunk.eval()

        with torch.no_grad():
            eval_loss = 0
            count = 0
            predictions = torch.empty((document_representations_sentence.shape[0], 11))
            
            for i in range(0, document_representations_sentence.shape[0], self.args['batch_size']):
                # ë°°ì¹˜ ì²˜ë¦¬
                batch_document_tensors_sentence = document_representations_sentence[i:i + self.args['batch_size']].to(
                    device=self.args['device'])
                batch_predictions_sentence = self.bert_regression_by_chunk(
                    batch_document_tensors_sentence, device=self.args['device'])
                
                batch_document_tensors_full = document_representations_full[i:i + self.args['batch_size']].to(
                    device=self.args['device'])
                batch_predictions_full = self.bert_regression_by_word_document(
                    batch_document_tensors_full, device=self.args['device'])
                
                # ì•™ìƒë¸” ì˜ˆì¸¡ (ê°€ì¤‘ í‰ê· )
                batch_predictions_combined = 0.6 * batch_predictions_sentence + 0.4 * batch_predictions_full
                predictions[i:i + self.args['batch_size']] = batch_predictions_combined.cpu()
                
                # ì†ì‹¤ ê³„ì‚° (ê°„ì†Œí™”ëœ ë²„ì „)
                batch_labels = correct_output[i:i + self.args['batch_size']].to(device=self.args['device'])
                
                # ê· í˜• ì¡íŒ ì†ì‹¤ ì‚¬ìš©
                balanced_loss = balanced_multi_criterion_loss(batch_predictions_combined, batch_labels)
                eval_loss += balanced_loss.item()
                count += 1
                
            eval_loss /= count

        # ê²°ê³¼ ì €ì¥
        if writeflag:
            outfile = open(os.path.join(self.args['model_directory'], self.args['result_file']), "w", encoding='utf-8')
            for i in range(predictions.shape[0]):
                true_scores = correct_output[i].numpy()
                pred_scores = predictions[i].numpy()
                outfile.write(f"Sample {i}:\n")
                for j, criterion in enumerate(self.criterion_names):
                    outfile.write(f"  {criterion}: True={true_scores[j]:.3f}, Pred={pred_scores[j]:.3f}\n")
                outfile.write("\n")
            outfile.close()

        # í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°
        predictions_np = predictions.numpy()
        true_labels_np = correct_output.numpy()
        
        overall_mse = mean_squared_error(true_labels_np.flatten(), predictions_np.flatten())
        overall_mae = mean_absolute_error(true_labels_np.flatten(), predictions_np.flatten())
        overall_qwk = quadratic_weighted_kappa_multi(np.round(true_labels_np).astype(int).flatten(), 
                                                   np.round(predictions_np).astype(int).flatten())

        # ê° í‰ê°€ ê¸°ì¤€ë³„ ë©”íŠ¸ë¦­
        criterion_mse = []
        criterion_mae = []
        criterion_qwk = []
        for i in range(11):
            mse = mean_squared_error(true_labels_np[:, i], predictions_np[:, i])
            mae = mean_absolute_error(true_labels_np[:, i], predictions_np[:, i])
            qwk = quadratic_weighted_kappa_multi(np.round(true_labels_np[:,i]).astype(int), 
                                               np.round(predictions_np[:,i]).astype(int))
            criterion_mse.append(mse)
            criterion_mae.append(mae)
            criterion_qwk.append(qwk)
        
        print(f"Overall MSE: {overall_mse:.4f} Overall MAE: {overall_mae:.4f} Overall QWK: {overall_qwk:.4f}")
        
        for i, criterion in enumerate(self.criterion_names):
            print(f"{criterion} - MSE: {criterion_mse[i]:.4f}, MAE: {criterion_mae[i]:.4f}, QWK: {criterion_qwk[i]:.4f}")
        
        return overall_mse, overall_mae, (true_labels_np, predictions_np), overall_qwk, eval_loss, criterion_mse, criterion_mae, criterion_qwk

    def fit(self, data_, test=None, mode='train', patience=8, log_dir='./logs'):
        """ê°œì„ ëœ ë©€í‹°-íƒœìŠ¤í¬ íšŒê·€ í•™ìŠµ (ë²„ê·¸ ìˆ˜ì • ë° ë¶ˆê· í˜• ë°ì´í„° ëŒ€ì‘)"""
        # ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
        lr = 3e-4  # í•™ìŠµë¥  ë” ì¦ê°€
        epochs = 16  # ì—í¬í¬ ìˆ˜ ì¡°ì •
        weight_decay = 0.01
        warmup_steps_ratio = self.args['warmup_ratio']
        
        # ë¡œê·¸ ì„¤ì •
        os.makedirs(log_dir, exist_ok=True)
        log_file = os.path.join(log_dir, f'training_log_{mode}_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt')
        
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write(f"ìˆ˜ì •ëœ Training Log - {mode} mode\n")
            f.write(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Learning Rate: {lr}\n")
            f.write(f"Max Epochs: {epochs}\n")
            f.write(f"Weight Decay: {weight_decay}\n")
            f.write(f"Patience: {patience}\n")
            f.write(f"Batch Size: {self.args['batch_size']}\n")
            f.write(f"Gradient Accumulation Steps: {self.args['gradient_accumulation_steps']}\n")
            f.write("="*80 + "\n\n")
        
        model_save_dir = './models'
        doc_model_save_dir = '{}/doc_model'.format(model_save_dir)
        chunk_model_save_dir = "{}/chunk_model".format(model_save_dir)
        
        # ê°œì„ ëœ ì˜µí‹°ë§ˆì´ì € ì„¤ì •
        optimizer = torch.optim.AdamW([
            {'params': self.bert_regression_by_word_document.parameters(), 'lr': lr * 0.1},  # BERTëŠ” ë‚®ì€ í•™ìŠµë¥ 
            {'params': self.bert_regression_by_chunk.parameters(), 'lr': lr * 0.1},
            {'params': self.adaptive_loss.parameters(), 'lr': lr * 2}  # ì†ì‹¤ ê°€ì¤‘ì¹˜ëŠ” ë†’ì€ í•™ìŠµë¥ 
        ], weight_decay=weight_decay)
        
        # êµì°¨ ê²€ì¦ ëŒ€ì‹  ë‹¨ìˆœ train/validation split ì‚¬ìš© (ì†ë„ í–¥ìƒ)
        loss_list = []
        mse_list = []
        mae_list = []
        
        def log_message(message):
            print(message)
            with open(log_file, 'a', encoding='utf-8') as f:
                f.write(f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {message}\n")
        
        # ë°ì´í„° ì¤€ë¹„
        train_essays = data_[0].tolist()
        train_labels = data_[1].values
        
        if test:
            test_essays = test[0].tolist()
            test_labels = test[1].values
            test_data_tuple = (test_essays, test_labels)
        
        log_message(f"í›ˆë ¨ ë°ì´í„°: {len(train_essays)}ê°œ")
        
        # ë°ì´í„° ì¸ì½”ë”©
        log_message("ë°ì´í„° ì¸ì½”ë”© ì‹œì‘...")
        encoding_start_time = time.time()
        
        document_representations_sentence, _ = encode_documents_by_sentence(
            train_essays, self.bert_tokenizer, max_input_length=self.max_sentence_length)
        document_representations_full, _ = encode_documents_full_text(
            train_essays, self.bert_tokenizer, max_input_length=self.max_doc_length)
        
        correct_output = torch.FloatTensor(train_labels)
        
        encoding_time = time.time() - encoding_start_time
        log_message(f"ë°ì´í„° ì¸ì½”ë”© ì™„ë£Œ (ì†Œìš”ì‹œê°„: {encoding_time:.2f}ì´ˆ)")
        
        # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.bert_regression_by_word_document.to(device=self.args['device'])
        self.bert_regression_by_chunk.to(device=self.args['device'])
        self.adaptive_loss.to(device=self.args['device'])
        
        self.bert_regression_by_word_document.train()
        self.bert_regression_by_chunk.train()
        self.adaptive_loss.train()
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        total_steps = (len(train_essays) // (self.args['batch_size'] * self.args['gradient_accumulation_steps'])) * epochs
        warmup_steps = int(total_steps * warmup_steps_ratio)
        
        from transformers import get_linear_schedule_with_warmup
        scheduler = get_linear_schedule_with_warmup(
            optimizer, 
            num_warmup_steps=warmup_steps,
            num_training_steps=total_steps
        )
        
        # Early stopping ì´ˆê¸°í™”
        best_qwk = -np.inf
        patience_counter = 0
        early_stop = False
        
        # í•™ìŠµ ë£¨í”„
        for epoch in range(1, epochs + 1):
            if early_stop:
                log_message(f"Early stopping triggered at epoch {epoch-1}")
                break
                
            epoch_start_time = time.time()
            epoch_loss = 0
            num_batches = 0
            
            log_message(f"--- Epoch {epoch}/{epochs} ì‹œì‘ ---")
            
            # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ìœ„í•œ ì´ˆê¸°í™”
            optimizer.zero_grad()
            accumulated_loss = 0
            
            for i in range(0, document_representations_sentence.shape[0], self.args['batch_size']):
                batch_start_time = time.time()
                
                # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
                batch_sentence = document_representations_sentence[i:i + self.args['batch_size']].to(
                    device=self.args['device'])
                batch_full = document_representations_full[i:i + self.args['batch_size']].to(
                    device=self.args['device'])
                batch_labels = correct_output[i:i + self.args['batch_size']].to(
                    device=self.args['device'])
                
                # Forward pass
                predictions_sentence = self.bert_regression_by_chunk(batch_sentence, device=self.args['device'])
                predictions_full = self.bert_regression_by_word_document(batch_full, device=self.args['device'])
                
                # ì•™ìƒë¸” ì˜ˆì¸¡
                combined_predictions = 0.6 * predictions_sentence + 0.4 * predictions_full
                
                # ì†ì‹¤ ê³„ì‚° (ìˆ˜ì •ëœ ë²„ì „ - íƒ€ì… ì•ˆì „ì„± ë³´ì¥)
                try:
                    # ì£¼ ì†ì‹¤: ê· í˜• ì¡íŒ MSE
                    balanced_mse = balanced_multi_criterion_loss(combined_predictions, batch_labels)
                    
                    # ë³´ì¡° ì†ì‹¤ë“¤ - íƒ€ì… ì²´í¬ ë° ë³€í™˜
                    sim_loss_val = improved_sim_loss(combined_predictions, batch_labels)
                    mr_loss_val = improved_mr_loss_func(combined_predictions, batch_labels)
                    
                    # ëª¨ë“  ì†ì‹¤ì´ í…ì„œì¸ì§€ í™•ì¸
                    if not isinstance(balanced_mse, torch.Tensor):
                        balanced_mse = torch.tensor(balanced_mse, device=self.args['device'], dtype=combined_predictions.dtype)
                    if not isinstance(sim_loss_val, torch.Tensor):
                        sim_loss_val = torch.tensor(sim_loss_val, device=self.args['device'], dtype=combined_predictions.dtype)
                    if not isinstance(mr_loss_val, torch.Tensor):
                        mr_loss_val = torch.tensor(mr_loss_val, device=self.args['device'], dtype=combined_predictions.dtype)
                    
                    # ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ ì ìš©
                    losses = torch.stack([balanced_mse, sim_loss_val, mr_loss_val])
                    total_loss, current_weights = self.adaptive_loss(losses)
                    
                except Exception as e:
                    log_message(f"ì†ì‹¤ ê³„ì‚° ì˜¤ë¥˜: {e}, ê¸°ë³¸ MSE ì‚¬ìš©")
                    total_loss = F.mse_loss(combined_predictions, batch_labels)
                    current_weights = torch.tensor([1.0, 0.0, 0.0])
                    balanced_mse = total_loss
                    sim_loss_val = torch.tensor(0.0)
                    mr_loss_val = torch.tensor(0.0)
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì ì„ ìœ„í•´ ì •ê·œí™”
                total_loss = total_loss / self.args['gradient_accumulation_steps']
                total_loss.backward()
                
                accumulated_loss += total_loss.item()
                
                # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í…ë§ˆë‹¤ ì—…ë°ì´íŠ¸
                if (num_batches + 1) % self.args['gradient_accumulation_steps'] == 0:
                    # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘
                    torch.nn.utils.clip_grad_norm_(
                        list(self.bert_regression_by_word_document.parameters()) +
                        list(self.bert_regression_by_chunk.parameters()) +
                        list(self.adaptive_loss.parameters()),
                        max_norm=self.args['max_grad_norm']
                    )
                    
                    optimizer.step()
                    scheduler.step()
                    optimizer.zero_grad()
                    
                    epoch_loss += accumulated_loss
                    accumulated_loss = 0
                
                num_batches += 1
                batch_time = time.time() - batch_start_time
                
                # ë°°ì¹˜ë³„ ìƒì„¸ ë¡œê¹… (ë§¤ 20 ë°°ì¹˜ë§ˆë‹¤)
                if num_batches % 20 == 0:
                    current_lr = scheduler.get_last_lr()[0]
                    log_message(f"  Batch {num_batches}: Loss={total_loss.item()*self.args['gradient_accumulation_steps']:.4f} "
                            f"(Balanced_MSE={balanced_mse.item():.4f}, Sim={sim_loss_val.item():.4f}, "
                            f"MR={mr_loss_val.item():.4f}) "
                            f"Weights=[{current_weights[0]:.3f}, {current_weights[1]:.3f}, {current_weights[2]:.3f}] "
                            f"LR={current_lr:.2e} Time={batch_time:.2f}s")
            
            # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
            if accumulated_loss > 0:
                torch.nn.utils.clip_grad_norm_(
                    list(self.bert_regression_by_word_document.parameters()) +
                    list(self.bert_regression_by_chunk.parameters()) +
                    list(self.adaptive_loss.parameters()),
                    max_norm=self.args['max_grad_norm']
                )
                optimizer.step()
                scheduler.step()
                optimizer.zero_grad()
                epoch_loss += accumulated_loss
            
            effective_batches = max(1, num_batches // self.args['gradient_accumulation_steps'])
            epoch_loss /= effective_batches
            loss_list.append(epoch_loss)
            epoch_time = time.time() - epoch_start_time
            
            current_lr = scheduler.get_last_lr()[0]
            log_message(f'Epoch {epoch} ì™„ë£Œ - '
                    f'Loss: {epoch_loss:.4f}, '
                    f'LR: {current_lr:.2e}, '
                    f'Time: {epoch_time:.2f}s')
            
            # ê²€ì¦
            if test:
                eval_start_time = time.time()
                log_message("ê²€ì¦ ì‹œì‘...")
                
                overall_mse, overall_mae, _, overall_qwk, eval_loss, criterion_mse, criterion_mae, criterion_qwk = self.predict_for_regress(test_data_tuple)
                mse_list.append(overall_mse)
                mae_list.append(overall_mae)
                
                eval_time = time.time() - eval_start_time
                
                log_message(f"ê²€ì¦ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {eval_time:.2f}ì´ˆ)")
                log_message(f"Overall - MSE: {overall_mse:.4f}, MAE: {overall_mae:.4f}, "
                        f"QWK: {overall_qwk:.4f}, Eval Loss: {eval_loss:.4f}")
                
                # ì–´ë ¤ìš´ ê¸°ì¤€ë“¤ì˜ ì„±ëŠ¥ ì¶œë ¥
                difficult_indices = [2, 4, 9]  # ë¬¸ì¥ í‘œí˜„, ë¬¸ë‹¨ ê°„ êµ¬ì¡°, í”„ë¡¬í”„íŠ¸ ë…í•´ë ¥
                difficult_qwk = np.mean([criterion_qwk[i] for i in difficult_indices])
                log_message(f"ì–´ë ¤ìš´ ê¸°ì¤€ í‰ê·  QWK: {difficult_qwk:.4f}")
                
                # ëª¨ë¸ ì €ì¥ ë¡œì§
                save_flag = False
                improvement_msg = ""
                
                # QWK ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨
                if overall_qwk > best_qwk:
                    best_qwk = overall_qwk
                    save_flag = True
                    improvement_msg = f"QWK ê°œì„  ({overall_qwk:.4f})"
                    patience_counter = 0
                else:
                    patience_counter += 1
                    improvement_msg = f"ì„±ëŠ¥ ê°œì„  ì—†ìŒ (patience: {patience_counter}/{patience})"
                
                if save_flag:
                    if not os.path.exists(model_save_dir):
                        os.makedirs(model_save_dir)
                    self.bert_regression_by_word_document.save_pretrained(doc_model_save_dir)
                    self.bert_regression_by_chunk.save_pretrained(chunk_model_save_dir)
                    torch.save(self.adaptive_loss.state_dict(), os.path.join(model_save_dir, 'adaptive_loss.pt'))
                    log_message(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {improvement_msg}")
                else:
                    log_message(improvement_msg)
                
                # Early stopping ì²´í¬
                if patience_counter >= patience:
                    log_message(f"Early stopping: {patience} epochs ë™ì•ˆ ì„±ëŠ¥ ê°œì„  ì—†ìŒ")
                    early_stop = True
                
                # ë‹¤ì‹œ í•™ìŠµ ëª¨ë“œë¡œ ë³€ê²½
                self.bert_regression_by_word_document.train()
                self.bert_regression_by_chunk.train()
                self.adaptive_loss.train()
        
        # ì „ì²´ í•™ìŠµ ì™„ë£Œ í›„ ê²°ê³¼ ì €ì¥
        os.makedirs('./train_valid_loss', exist_ok=True)
        np.save(f'./train_valid_loss/{mode}_loss.npy', np.array(loss_list))
        np.save(f'./train_valid_loss/{mode}_mse.npy', np.array(mse_list))
        np.save(f'./train_valid_loss/{mode}_mae.npy', np.array(mae_list))
        
        final_mse = np.mean(mse_list) if mse_list else float('inf')
        final_mae = np.mean(mae_list) if mae_list else float('inf')
        
        log_message("="*80)
        log_message("ìˆ˜ì •ëœ í•™ìŠµ ì™„ë£Œ!")
        log_message(f"ìµœì¢… í‰ê·  MSE: {final_mse:.4f}")
        log_message(f"ìµœì¢… í‰ê·  MAE: {final_mae:.4f}")
        log_message(f"ìµœê³  QWK: {best_qwk:.4f}")
        log_message(f"ë¡œê·¸ íŒŒì¼ ì €ì¥ ìœ„ì¹˜: {log_file}")
        log_message(f"ì¢…ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        print(f"Average MSE: {final_mse:.4f}")
        print(f"Average MAE: {final_mae:.4f}")
        print(f"Best QWK: {best_qwk:.4f}")
        print(f"Training log saved to: {log_file}")

    def predict_single(self, input_sentence):
        """ë‹¨ì¼ ë¬¸ì¥ì— ëŒ€í•œ 11ê°œ í‰ê°€ ê¸°ì¤€ ì˜ˆì¸¡"""
        # ë¬¸ì¥ë³„ ì¸ì½”ë”©
        document_representations_sentence, _ = encode_documents_by_sentence(
            [input_sentence], self.bert_tokenizer, max_input_length=self.max_sentence_length)
        
        # ì „ì²´ ë¬¸ì„œ ì¸ì½”ë”©
        document_representations_full, _ = encode_documents_full_text(
            [input_sentence], self.bert_tokenizer, max_input_length=self.max_doc_length)
        
        # ëª¨ë¸ì„ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
        self.bert_regression_by_word_document.to(device=self.args['device'])
        self.bert_regression_by_chunk.to(device=self.args['device'])
        
        self.bert_regression_by_word_document.eval()
        self.bert_regression_by_chunk.eval()
        
        with torch.no_grad():
            # ì˜ˆì¸¡
            sentence_tensor = document_representations_sentence.to(device=self.args['device'])
            full_tensor = document_representations_full.to(device=self.args['device'])
            
            predictions_sentence = self.bert_regression_by_chunk(sentence_tensor, device=self.args['device'])
            predictions_full = self.bert_regression_by_word_document(full_tensor, device=self.args['device'])
            
            # ì•™ìƒë¸” ê²°í•© (ê°œì„ ëœ ê°€ì¤‘ì¹˜)
            combined_predictions = 0.6 * predictions_sentence + 0.4 * predictions_full
            predictions = combined_predictions.cpu().numpy()[0]  # (11,)
        
        # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
        result_dict = {}
        for i, criterion in enumerate(self.criterion_names):
            result_dict[criterion] = float(predictions[i])
        
        return result_dict, predictions
    
    def load_best_model(self, model_dir='./models'):
        """ì €ì¥ëœ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ"""
        doc_model_path = os.path.join(model_dir, 'doc_model')
        chunk_model_path = os.path.join(model_dir, 'chunk_model')
        adaptive_loss_path = os.path.join(model_dir, 'adaptive_loss.pt')
        
        if os.path.exists(doc_model_path) and os.path.exists(chunk_model_path):
            print("ì €ì¥ëœ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
            self.bert_regression_by_word_document = DocumentBertCombineWordDocumentLinear.from_pretrained(
                doc_model_path, config=self.config)
            self.bert_regression_by_chunk = DocumentBertSentenceChunkAttentionLSTM.from_pretrained(
                chunk_model_path, config=self.config)
            
            if os.path.exists(adaptive_loss_path):
                self.adaptive_loss.load_state_dict(torch.load(adaptive_loss_path))
                print("ì ì‘ì  ì†ì‹¤ ê°€ì¤‘ì¹˜ë„ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            print("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
        else:
            print("ì €ì¥ëœ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    def analyze_training_progress(self, mode='train'):
        """í•™ìŠµ ì§„í–‰ ìƒí™© ë¶„ì„ ë° ì‹œê°í™”"""
        try:
            import matplotlib.pyplot as plt
            
            loss_path = f'./train_valid_loss/{mode}_loss.npy'
            mse_path = f'./train_valid_loss/{mode}_mse.npy'
            mae_path = f'./train_valid_loss/{mode}_mae.npy'
            
            if all(os.path.exists(p) for p in [loss_path, mse_path, mae_path]):
                losses = np.load(loss_path)
                mses = np.load(mse_path)
                maes = np.load(mae_path)
                
                fig, axes = plt.subplots(1, 3, figsize=(15, 5))
                
                axes[0].plot(losses)
                axes[0].set_title('Training Loss')
                axes[0].set_xlabel('Epoch')
                axes[0].set_ylabel('Loss')
                axes[0].grid(True)
                
                axes[1].plot(mses)
                axes[1].set_title('Validation MSE')
                axes[1].set_xlabel('Epoch')
                axes[1].set_ylabel('MSE')
                axes[1].grid(True)
                
                axes[2].plot(maes)
                axes[2].set_title('Validation MAE')
                axes[2].set_xlabel('Epoch')
                axes[2].set_ylabel('MAE')
                axes[2].grid(True)
                
                plt.tight_layout()
                plt.savefig(f'./logs/training_progress_{mode}.png', dpi=300, bbox_inches='tight')
                plt.show()
                
                print(f"í•™ìŠµ ì§„í–‰ ìƒí™© ê·¸ë˜í”„ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: ./logs/training_progress_{mode}.png")
            else:
                print("í•™ìŠµ ê¸°ë¡ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except ImportError:
            print("matplotlibì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        except Exception as e:
            print(f"ì‹œê°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    def analyze_data_imbalance(self, train_labels):
        """ë°ì´í„° ë¶ˆê· í˜• ë¶„ì„"""
        train_array = np.array(train_labels)
        
        print("\n" + "="*60)
        print("ğŸ“Š ë°ì´í„° ë¶ˆê· í˜• ë¶„ì„ ë° ëŒ€ì‘ ì „ëµ")
        print("="*60)
        
        # ê¸°ì¤€ë³„ ë¶„ì„
        easy_criteria = []
        medium_criteria = []
        hard_criteria = []
        
        for i, name in enumerate(self.criterion_names):
            mean_score = np.mean(train_array[:, i])
            std_score = np.std(train_array[:, i])
            
            # ë‚œì´ë„ ë¶„ë¥˜
            if mean_score >= 2.5 and std_score <= 0.5:
                easy_criteria.append((i, name, mean_score, std_score))
            elif mean_score <= 1.5 or std_score >= 1.2:
                hard_criteria.append((i, name, mean_score, std_score))
            else:
                medium_criteria.append((i, name, mean_score, std_score))
        
        print(f"ğŸŸ¢ ì‰¬ìš´ ê¸°ì¤€ ({len(easy_criteria)}ê°œ):")
        for idx, name, mean, std in easy_criteria:
            print(f"   {name}: í‰ê· ={mean:.3f}, í‘œì¤€í¸ì°¨={std:.3f}")
        
        print(f"\nğŸŸ¡ ë³´í†µ ê¸°ì¤€ ({len(medium_criteria)}ê°œ):")
        for idx, name, mean, std in medium_criteria:
            print(f"   {name}: í‰ê· ={mean:.3f}, í‘œì¤€í¸ì°¨={std:.3f}")
        
        print(f"\nğŸ”´ ì–´ë ¤ìš´ ê¸°ì¤€ ({len(hard_criteria)}ê°œ):")
        for idx, name, mean, std in hard_criteria:
            print(f"   {name}: í‰ê· ={mean:.3f}, í‘œì¤€í¸ì°¨={std:.3f}")
        
        # ëŒ€ì‘ ì „ëµ ì¶œë ¥
        print(f"\nğŸ¯ ì ìš©ëœ ëŒ€ì‘ ì „ëµ:")
        print("1. ê· í˜• ì¡íŒ ì†ì‹¤ í•¨ìˆ˜: ì–´ë ¤ìš´ ê¸°ì¤€ì— 1.5ë°° ê°€ì¤‘ì¹˜")
        print("2. ë™ì  ì†ì‹¤ ê°€ì¤‘ì¹˜: í•™ìŠµ ì¤‘ ìë™ ì¡°ì •")
        print("3. ì•™ìƒë¸” ëª¨ë¸: ë¬¸ì¥ë³„ + ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬")
        print("4. ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘: ì•ˆì •ì ì¸ í•™ìŠµ")
        
        return easy_criteria, medium_criteria, hard_criteria